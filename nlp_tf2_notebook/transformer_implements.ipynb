{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                              as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=40\n",
    "def filter_long_sent(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "        lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "        lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 使用.map()运行相关图操作\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "# 过滤过长的数据\n",
    "train_dataset = train_dataset.filter(filter_long_sent)\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 验证集数据\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_long_sent).padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    position_matrix = np.zeros((position, d_model))\n",
    "    for p in range(position):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                position_matrix[p][j] = np.sin(p/np.power(10000, 2*(j//2)/np.float32(d_model)))\n",
    "            else:\n",
    "                position_matrix[p][j] = np.cos(p/np.power(10000, 2*(j//2)/np.float32(d_model)))\n",
    "                \n",
    "    position_matrix = position_matrix[np.newaxis, ...]           \n",
    "    \n",
    "    return tf.cast(position_matrix, dtype=tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    # 这里的i等价与上面公式中的2i和2i+1\n",
    "    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding1(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    # 第2i项使用sin\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    # 第2i+1项使用cos\n",
    "    cones = np.cos(angle_rads[:, 1::2])\n",
    "    print(sines.shape)\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    print(pos_encoding.shape)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207419, shape=(1, 2, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 1.        ],\n",
       "        [0.84147096, 0.5403023 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding1(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 1.        ],\n",
       "        [0.84147098, 0.54030231]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.7320508>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = tf.constant([[2, 3, 4], [3, 4, 5]], dtype=tf.float32)\n",
    "a = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "tf.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scala_dot_product_attention(q, k, v, mask):\n",
    "    matmual_k = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmual_k/tf.sqrt(dk)\n",
    "    \n",
    "    attention_weight = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    \n",
    "    output = tf.matmul(attention_weight, v)\n",
    "    \n",
    "    return output, attention_weight\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([[1, 2, 3], [2, 3, 4]], dtype=tf.float32)\n",
    "k = tf.constant([[2, 3, 4], [3, 4, 5]], dtype=tf.float32)\n",
    "v = tf.constant([[3, 4, 5], [4, 5, 6]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[3.9696488, 4.969649 , 5.969649 ],\n",
       "        [3.9944925, 4.9944925, 5.994493 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.03035108, 0.9696489 ],\n",
       "        [0.00550734, 0.99449265]], dtype=float32)>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_dot_product_attention(q, k, v, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # d_model 必须可以正确分为各个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头后的维度\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头, 将头个数的维度 放到 seq_len 前面\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 分头前的前向网络，获取q、k、v语义\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 通过缩放点积注意力层\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "        # 合并多头\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512) (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, d_head=8)\n",
    "y = tf.random.uniform((1, 60, 512))\n",
    "output, att = temp_mha(y, y, y, mask=None)\n",
    "print(output.shape, att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, diff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(diff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_fnn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_fnn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        self.eps = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ddf, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ddf)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        # 多头注意力网络\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # 前向网络\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, drop_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(drop_rate)\n",
    "        self.dropout2 = layers.Dropout(drop_rate)\n",
    "        self.dropout3 = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self,inputs, encode_out, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "        # masked muti-head attention\n",
    "        att1, att_weight1 = self.mha1(inputs, inputs, inputs,look_ahead_mask)\n",
    "        att1 = self.dropout1(att1, training=training)\n",
    "        out1 = self.layernorm1(inputs + att1)\n",
    "        # muti-head attention\n",
    "        att2, att_weight2 = self.mha2(encode_out, encode_out, inputs, padding_mask)\n",
    "        att2 = self.dropout2(att2, training=training)\n",
    "        out2 = self.layernorm2(out1 + att2)\n",
    "        \n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_out)\n",
    "        \n",
    "        return out3, att_weight1, att_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "sample_decoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                input_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.encode_layer = [EncoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                            for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "    def call(self, inputs, training, mark):\n",
    "        \n",
    "        seq_len = inputs.shape[1]\n",
    "        word_emb = self.embedding(inputs)\n",
    "        word_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_emb + self.pos_embedding[:,:seq_len,:]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encode_layer[i](x, training, mark)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 120, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder = Encoder(2, 512, 8, 1024, 5000, 200)\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 120)),\n",
    "                                      False, None)\n",
    "sample_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                target_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.decoder_layers= [DecoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                             for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self, inputs, encoder_out,training,\n",
    "             look_ahead_mark, padding_mark):\n",
    "    \n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "        h = self.embedding(inputs)\n",
    "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        h += self.pos_embedding[:,:seq_len,:]\n",
    "        \n",
    "        h = self.dropout(h, training=training)\n",
    "#         print('--------------------\\n',h, h.shape)\n",
    "        # 叠加解码层\n",
    "        for i in range(self.n_layers):\n",
    "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
    "                                                   training, look_ahead_mark,\n",
    "                                                   padding_mark)\n",
    "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
    "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
    "        \n",
    "        return h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 100, 512]), TensorShape([64, 8, 100, 100]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(2, 512,8,1024,5000, 200)\n",
    "sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),\n",
    "                                            sample_encoder_output, False,\n",
    "                                            None, None)\n",
    "sample_decoder_output.shape, attn['decoder_layer1_att_w1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, diff,\n",
    "                input_vocab_size, target_vocab_size,\n",
    "                max_seq_len, drop_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads,diff,\n",
    "                              input_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, diff,\n",
    "                              target_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs, targets, training, encode_padding_mask, \n",
    "            look_ahead_mask, decode_padding_mask):\n",
    "        \n",
    "        encode_out = self.encoder(inputs, training, encode_padding_mask)\n",
    "        print(encode_out.shape)\n",
    "        decode_out, att_weights = self.decoder(targets, encode_out, training, \n",
    "                                               look_ahead_mask, decode_padding_mask)\n",
    "        print(decode_out.shape)\n",
    "        final_out = self.final_layer(decode_out)\n",
    "        \n",
    "        return final_out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n",
      "(64, 26, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "n_layers=2, d_model=512, n_heads=8, diff=1024,\n",
    "input_vocab_size=8500, target_vocab_size=8000, max_seq_len=120\n",
    ")\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                              encode_padding_mask=None,\n",
    "                               look_ahead_mask=None,\n",
    "                               decode_padding_mask=None,\n",
    "                              )\n",
    "fn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                              as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=40\n",
    "def filter_long_sent(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "        lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "        lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .', shape=(), dtype=string)\n",
      "tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n",
      "[8214, 6, 40, 4092, 57, 3, 1687, 1, 6155, 12, 3, 461, 6770, 19, 5227, 1088, 97, 1, 5, 8, 3, 4213, 3408, 7256, 1670, 2, 8215]\n",
      "[8087, 4, 59, 15, 1792, 6561, 3060, 7952, 1, 15, 103, 134, 378, 3, 47, 6122, 6, 5311, 1, 91, 13, 1849, 559, 1609, 894, 2, 8088]\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_examples:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    x, y = encode(x, y)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 使用.map()运行相关图操作\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "# 过滤过长的数据\n",
    "train_dataset = train_dataset.filter(filter_long_sent)\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 验证集数据\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_long_sent).padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "max_seq_len = 40\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, \n",
    "                                    beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'train step')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试\n",
    "temp_learing_rate = CustomSchedule(d_model)\n",
    "plt.plot(temp_learing_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('train step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size,\n",
    "                          max_seq_len, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码\n",
    "def create_mask(inputs,targets):\n",
    "    encode_padding_mask = create_padding_mark(inputs)\n",
    "    # 这个掩码用于掩输入解码层第二层的编码层输出\n",
    "    decode_padding_mask = create_padding_mark(inputs)\n",
    "    \n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "    \n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return encode_padding_mask, combine_mask, decode_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last checkpoit restore\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './checkpoint/train'\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    tar_inp = targets[:,:-1]\n",
    "    tar_real = targets[:,1:]\n",
    "    # 构造掩码\n",
    "    encode_padding_mask, combined_mask, decode_padding_mask = create_mask(inputs, tar_inp)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inputs, tar_inp,\n",
    "                                    True,\n",
    "                                    encode_padding_mask,\n",
    "                                    combined_mask,\n",
    "                                    decode_padding_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mark(seq):\n",
    "    # 获取为0的padding项\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # 扩充维度以便用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)\n",
    "\n",
    "# mark 测试\n",
    "create_padding_mark([[1,2,0,0,3],[3,4,5,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mark(size):\n",
    "    # 1 - 对角线和取下三角的全部对角线（-1->全部）\n",
    "    # 这样就可以构造出每个时刻未预测token的掩码\n",
    "    mark = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mark  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "epoch 1, batch 0, loss:3.6506, acc:0.0000\n",
      "epoch 1, batch 500, loss:2.6984, acc:0.0592\n",
      "(31, 40, 128)\n",
      "(31, 39, 128)\n",
      "epoch 1, loss:2.4515, acc:0.0956\n",
      "time in 1 epoch:820.1156380176544 secs\n",
      "\n",
      "epoch 2, batch 0, loss:1.2324, acc:0.2280\n",
      "epoch 2, batch 500, loss:0.9284, acc:0.3113\n",
      "epoch 2, loss:0.8000, acc:0.3304\n",
      "time in 1 epoch:791.5252058506012 secs\n",
      "\n",
      "epoch 3, batch 0, loss:0.3320, acc:0.3562\n",
      "epoch 3, batch 500, loss:0.2521, acc:0.4142\n",
      "epoch 3, loss:0.2140, acc:0.4211\n",
      "time in 1 epoch:748.6557490825653 secs\n",
      "\n",
      "epoch 4, batch 0, loss:0.0842, acc:0.3970\n",
      "epoch 4, batch 500, loss:0.0568, acc:0.4437\n",
      "epoch 4, loss:0.0488, acc:0.4450\n",
      "time in 1 epoch:764.3711895942688 secs\n",
      "\n",
      "epoch 5, batch 0, loss:0.0274, acc:0.4026\n",
      "epoch 5, batch 500, loss:0.0190, acc:0.4462\n",
      "epoch 5, loss:0.0175, acc:0.4471\n",
      "time in 1 epoch:744.6127412319183 secs\n",
      "\n",
      "epoch 6, batch 0, loss:0.0090, acc:0.4046\n",
      "epoch 6, batch 500, loss:0.0117, acc:0.4466\n",
      "epoch 6, loss:0.0115, acc:0.4474\n",
      "time in 1 epoch:744.1425678730011 secs\n",
      "\n",
      "epoch 7, batch 0, loss:0.0041, acc:0.4054\n",
      "epoch 7, batch 500, loss:0.0103, acc:0.4467\n",
      "epoch 7, loss:0.0100, acc:0.4475\n",
      "time in 1 epoch:773.6023874282837 secs\n",
      "\n",
      "epoch 8, batch 0, loss:0.0065, acc:0.4046\n",
      "epoch 8, batch 500, loss:0.0085, acc:0.4468\n",
      "epoch 8, loss:0.0083, acc:0.4476\n",
      "time in 1 epoch:762.0688850879669 secs\n",
      "\n",
      "epoch 9, batch 0, loss:0.0094, acc:0.4038\n",
      "epoch 9, batch 500, loss:0.0073, acc:0.4469\n",
      "epoch 9, loss:0.0074, acc:0.4477\n",
      "time in 1 epoch:753.4617078304291 secs\n",
      "\n",
      "epoch 10, batch 0, loss:0.0029, acc:0.4050\n",
      "epoch 10, batch 500, loss:0.0061, acc:0.4470\n",
      "epoch 10, loss:0.0061, acc:0.4478\n",
      "time in 1 epoch:752.8659400939941 secs\n",
      "\n",
      "epoch 11, batch 0, loss:0.0015, acc:0.4054\n",
      "epoch 11, batch 500, loss:0.0060, acc:0.4470\n",
      "epoch 11, loss:0.0058, acc:0.4478\n",
      "time in 1 epoch:813.7948746681213 secs\n",
      "\n",
      "epoch 12, batch 0, loss:0.0015, acc:0.4054\n",
      "epoch 12, batch 500, loss:0.0056, acc:0.4471\n",
      "epoch 12, loss:0.0057, acc:0.4479\n",
      "time in 1 epoch:825.3873047828674 secs\n",
      "\n",
      "epoch 13, batch 0, loss:0.0014, acc:0.4054\n",
      "epoch 13, batch 500, loss:0.0051, acc:0.4472\n",
      "epoch 13, loss:0.0052, acc:0.4479\n",
      "time in 1 epoch:866.0107836723328 secs\n",
      "\n",
      "epoch 14, batch 0, loss:0.0012, acc:0.4054\n",
      "epoch 14, batch 500, loss:0.0046, acc:0.4472\n",
      "epoch 14, loss:0.0045, acc:0.4480\n",
      "time in 1 epoch:861.3611538410187 secs\n",
      "\n",
      "epoch 15, batch 0, loss:0.0011, acc:0.4054\n",
      "epoch 15, batch 500, loss:0.0044, acc:0.4472\n",
      "epoch 15, loss:0.0045, acc:0.4480\n",
      "time in 1 epoch:843.6812822818756 secs\n",
      "\n",
      "epoch 16, batch 0, loss:0.0032, acc:0.4050\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # inputs 葡萄牙语， targets英语\n",
    "    \n",
    "    for batch, (inputs, targets) in enumerate(train_dataset):\n",
    "        # 训练\n",
    "        train_step(inputs, targets)\n",
    "        \n",
    "        if batch % 500 == 0:\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "            epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
    "            ))\n",
    "            \n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#         print('epoch {}, save model at {}'.format(\n",
    "#         epoch+1, ckpt_save_path\n",
    "#         ))\n",
    "    \n",
    "    \n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "    epoch+1, train_loss.result(), train_accuracy.result()\n",
    "    ))\n",
    "    \n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
