{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn, 即循环神经网络（recurrent neural network）. 相比全连接神经网络，可以更好的处理序列数据。对于序列数据，其中的每个元素是前后相关联的。全连接神经可以处理相对独立的数据，\n",
    "\n",
    "举个例子，对于词性标注问题，我/nn 吃/v 苹果/nn\n",
    "\n",
    "对于这个任务来说，那么全连接神经网络会如下的对应。我 -> nn 吃 -> v 苹果 -> nn\n",
    "很容易看到，对于词性标注问题，前面的词对于后面的词有影响，例如预测苹果的此行是，如果前面的吃是动词时，那么苹果作为名词的概率就会很大。因为动词后面接名词的概率很大。\n",
    "\n",
    "为了更好的处理的这样的问题，RNN就诞生了。\n",
    "\n",
    "RNN的网络结构如下\n",
    "输入层、隐藏层和输出层组成\n",
    "![title](img/rnn1.png)\n",
    "\n",
    "对于RNN的网络结构，其形态比较抽象。其中一个比较突出的特点就是隐藏层再次连接到隐藏层。这样的原理在于将上一个隐藏层的输出状态当成下一次输入的数据。就可以有效的利用序列的前一个状态信息。\n",
    "\n",
    "图中，W是RNN隐藏层的权重，V是隐藏层到输出层的权重，U是输入层到隐藏层的权重。\n",
    "\n",
    "输入值为X, 输出值为O, 隐藏状态为S\n",
    "\n",
    "那么\n",
    "S_i = f(UX_i + WS_i-1)\n",
    "O_i = g(VS_i)\n",
    "\n",
    "这样我们可以看到隐藏状态S_i， 和X_i以及S_i-1都是相关的\n",
    "\n",
    "对于rnn,其参数梯度的计算是使用TBP算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_target), (test_data, test_target) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_target = train_target.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padding = tf.keras.preprocessing.sequence.pad_sequences(train_data, padding=\"post\", maxlen=max_len)\n",
    "test_padding = tf.keras.preprocessing.sequence.pad_sequences(test_data, padding=\"post\", maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padding = train_padding.astype(\"float32\")\n",
    "test_padding = test_padding.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "EMBED_SIZE = 10\n",
    "RNN_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_padding, train_target))\n",
    "dataset = dataset.shuffle(100).batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JRNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(JRNN, self).__init__()\n",
    "        \n",
    "        self.embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=EMBED_SIZE)\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(RNN_SIZE, return_sequences=True, return_state=True)\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, input_x):\n",
    "        \n",
    "        x = self.embed(input_x)\n",
    "        out, state = self.rnn(x)\n",
    "        out = out[:,-1,:]\n",
    "        logits = self.out(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2909, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.50424653],\n",
       "       [0.49094072]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.constant([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(input_x, input_y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        logits = model(input_x)\n",
    "        loss = loss_func(logits, input_y)\n",
    "        \n",
    "    variables = model.variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss 0.7394543290138245\n",
      "epoch 0 batch 100 loss 0.5515481233596802\n",
      "epoch 0 batch 200 loss 0.5070056319236755\n",
      "epoch 1 batch 0 loss 0.5624309778213501\n",
      "epoch 1 batch 100 loss 0.5231655836105347\n",
      "epoch 1 batch 200 loss 0.49936443567276\n",
      "epoch 2 batch 0 loss 0.551193356513977\n",
      "epoch 2 batch 100 loss 0.5130713582038879\n",
      "epoch 2 batch 200 loss 0.5088028907775879\n",
      "epoch 3 batch 0 loss 0.5274919271469116\n",
      "epoch 3 batch 100 loss 0.5234302282333374\n",
      "epoch 3 batch 200 loss 0.4816802740097046\n",
      "epoch 4 batch 0 loss 0.5420442819595337\n",
      "epoch 4 batch 100 loss 0.5116667151451111\n",
      "epoch 4 batch 200 loss 0.5115427374839783\n",
      "epoch 5 batch 0 loss 0.5417777895927429\n",
      "epoch 5 batch 100 loss 0.5227643251419067\n",
      "epoch 5 batch 200 loss 0.5037571787834167\n",
      "epoch 6 batch 0 loss 0.5340408086776733\n",
      "epoch 6 batch 100 loss 0.5339873433113098\n",
      "epoch 6 batch 200 loss 0.5149919986724854\n",
      "epoch 7 batch 0 loss 0.5301385521888733\n",
      "epoch 7 batch 100 loss 0.5073539018630981\n",
      "epoch 7 batch 200 loss 0.5073172450065613\n",
      "epoch 8 batch 0 loss 0.526272177696228\n",
      "epoch 8 batch 100 loss 0.5110697746276855\n",
      "epoch 8 batch 200 loss 0.5148409605026245\n",
      "epoch 9 batch 0 loss 0.5186270475387573\n",
      "epoch 9 batch 100 loss 0.5034269094467163\n",
      "epoch 9 batch 200 loss 0.4692549407482147\n",
      "epoch 10 batch 0 loss 0.5299701690673828\n",
      "epoch 10 batch 100 loss 0.5223621129989624\n",
      "epoch 10 batch 200 loss 0.5185526609420776\n",
      "epoch 11 batch 0 loss 0.5337345600128174\n",
      "epoch 11 batch 100 loss 0.4957614243030548\n",
      "epoch 11 batch 200 loss 0.5071380734443665\n",
      "epoch 12 batch 0 loss 0.5488932132720947\n",
      "epoch 12 batch 100 loss 0.5374956727027893\n",
      "epoch 12 batch 200 loss 0.4919286072254181\n",
      "epoch 13 batch 0 loss 0.5260971784591675\n",
      "epoch 13 batch 100 loss 0.510901689529419\n",
      "epoch 13 batch 200 loss 0.510894775390625\n",
      "epoch 14 batch 0 loss 0.5336753129959106\n",
      "epoch 14 batch 100 loss 0.5184805989265442\n",
      "epoch 14 batch 200 loss 0.522271990776062\n",
      "epoch 15 batch 0 loss 0.5450547933578491\n",
      "epoch 15 batch 100 loss 0.5070764422416687\n",
      "epoch 15 batch 200 loss 0.49947601556777954\n",
      "epoch 16 batch 0 loss 0.5488426685333252\n",
      "epoch 16 batch 100 loss 0.5108634829521179\n",
      "epoch 16 batch 200 loss 0.5108590126037598\n",
      "epoch 17 batch 0 loss 0.5336444973945618\n",
      "epoch 17 batch 100 loss 0.480472207069397\n",
      "epoch 17 batch 200 loss 0.5184468030929565\n",
      "epoch 18 batch 0 loss 0.5374352931976318\n",
      "epoch 18 batch 100 loss 0.49565497040748596\n",
      "epoch 18 batch 200 loss 0.5070462226867676\n",
      "epoch 19 batch 0 loss 0.5374298691749573\n",
      "epoch 19 batch 100 loss 0.5108407139778137\n",
      "epoch 19 batch 200 loss 0.4994441866874695\n",
      "epoch 20 batch 0 loss 0.5260303616523743\n",
      "epoch 20 batch 100 loss 0.5222298502922058\n",
      "epoch 20 batch 200 loss 0.4956407845020294\n",
      "epoch 21 batch 0 loss 0.5298243165016174\n",
      "epoch 21 batch 100 loss 0.5108309388160706\n",
      "epoch 21 batch 200 loss 0.5260226726531982\n",
      "epoch 22 batch 0 loss 0.5374171733856201\n",
      "epoch 22 batch 100 loss 0.5184239745140076\n",
      "epoch 22 batch 200 loss 0.5108257532119751\n",
      "epoch 23 batch 0 loss 0.5336158275604248\n",
      "epoch 23 batch 100 loss 0.5146223902702332\n",
      "epoch 23 batch 200 loss 0.4956290125846863\n",
      "epoch 24 batch 0 loss 0.5336132645606995\n",
      "epoch 24 batch 100 loss 0.5184182524681091\n",
      "epoch 24 batch 200 loss 0.5260138511657715\n",
      "epoch 25 batch 0 loss 0.545006275177002\n",
      "epoch 25 batch 100 loss 0.522214412689209\n",
      "epoch 25 batch 200 loss 0.5108179450035095\n",
      "epoch 26 batch 0 loss 0.5298104286193848\n",
      "epoch 26 batch 100 loss 0.5032194256782532\n",
      "epoch 26 batch 200 loss 0.4918229579925537\n",
      "epoch 27 batch 0 loss 0.5374057292938232\n",
      "epoch 27 batch 100 loss 0.5298078656196594\n",
      "epoch 27 batch 200 loss 0.5070155262947083\n",
      "epoch 28 batch 0 loss 0.548799991607666\n",
      "epoch 28 batch 100 loss 0.4956187903881073\n",
      "epoch 28 batch 200 loss 0.5146113038063049\n",
      "epoch 29 batch 0 loss 0.5260071754455566\n",
      "epoch 29 batch 100 loss 0.5032145380973816\n",
      "epoch 29 batch 200 loss 0.45763006806373596\n",
      "epoch 30 batch 0 loss 0.5374019145965576\n",
      "epoch 30 batch 100 loss 0.49561604857444763\n",
      "epoch 30 batch 200 loss 0.5260049700737\n",
      "epoch 31 batch 0 loss 0.5222062468528748\n",
      "epoch 31 batch 100 loss 0.5222058296203613\n",
      "epoch 31 batch 200 loss 0.4842183291912079\n",
      "epoch 32 batch 0 loss 0.5411987900733948\n",
      "epoch 32 batch 100 loss 0.5298023819923401\n",
      "epoch 32 batch 200 loss 0.5184058547019958\n",
      "epoch 33 batch 0 loss 0.5260032415390015\n",
      "epoch 33 batch 100 loss 0.5070092678070068\n",
      "epoch 33 batch 200 loss 0.5070089101791382\n",
      "epoch 34 batch 0 loss 0.5525937676429749\n",
      "epoch 34 batch 100 loss 0.5032097697257996\n",
      "epoch 34 batch 200 loss 0.5260019898414612\n",
      "epoch 35 batch 0 loss 0.5487945079803467\n",
      "epoch 35 batch 100 loss 0.5146053433418274\n",
      "epoch 35 batch 200 loss 0.5260014533996582\n",
      "epoch 36 batch 0 loss 0.5487940311431885\n",
      "epoch 36 batch 100 loss 0.5108060836791992\n",
      "epoch 36 batch 200 loss 0.495610773563385\n",
      "epoch 37 batch 0 loss 0.5335984826087952\n",
      "epoch 37 batch 100 loss 0.5260007381439209\n",
      "epoch 37 batch 200 loss 0.5070066452026367\n",
      "epoch 38 batch 0 loss 0.5222017765045166\n",
      "epoch 38 batch 100 loss 0.49940887093544006\n",
      "epoch 38 batch 200 loss 0.46521949768066406\n",
      "epoch 39 batch 0 loss 0.5297989845275879\n",
      "epoch 39 batch 100 loss 0.5373963713645935\n",
      "epoch 39 batch 200 loss 0.4994083344936371\n",
      "epoch 40 batch 0 loss 0.5411950945854187\n",
      "epoch 40 batch 100 loss 0.5297985672950745\n",
      "epoch 40 batch 200 loss 0.5184019804000854\n",
      "epoch 41 batch 0 loss 0.5108043551445007\n",
      "epoch 41 batch 100 loss 0.5297982692718506\n",
      "epoch 41 batch 200 loss 0.5184017419815063\n",
      "epoch 42 batch 0 loss 0.5487922430038452\n",
      "epoch 42 batch 100 loss 0.5146028399467468\n",
      "epoch 42 batch 200 loss 0.5335968136787415\n",
      "epoch 43 batch 0 loss 0.522200345993042\n",
      "epoch 43 batch 100 loss 0.5222002267837524\n",
      "epoch 43 batch 200 loss 0.4956084191799164\n",
      "epoch 44 batch 0 loss 0.5411942601203918\n",
      "epoch 44 batch 100 loss 0.5222001075744629\n",
      "epoch 44 batch 200 loss 0.5146023035049438\n",
      "epoch 45 batch 0 loss 0.5373952388763428\n",
      "epoch 45 batch 100 loss 0.5146023035049438\n",
      "epoch 45 batch 200 loss 0.5070045590400696\n",
      "epoch 46 batch 0 loss 0.5601881146430969\n",
      "epoch 46 batch 100 loss 0.529797375202179\n",
      "epoch 46 batch 200 loss 0.5259984731674194\n",
      "epoch 47 batch 0 loss 0.5525903701782227\n",
      "epoch 47 batch 100 loss 0.5335960984230042\n",
      "epoch 47 batch 200 loss 0.5259984135627747\n",
      "epoch 48 batch 0 loss 0.5411937236785889\n",
      "epoch 48 batch 100 loss 0.5297971367835999\n",
      "epoch 48 batch 200 loss 0.5032052993774414\n",
      "epoch 49 batch 0 loss 0.5411936640739441\n",
      "epoch 49 batch 100 loss 0.5108029842376709\n",
      "epoch 49 batch 200 loss 0.4804121255874634\n",
      "epoch 50 batch 0 loss 0.5108028650283813\n",
      "epoch 50 batch 100 loss 0.5373947024345398\n",
      "epoch 50 batch 200 loss 0.5032050609588623\n",
      "epoch 51 batch 0 loss 0.5373947024345398\n",
      "epoch 51 batch 100 loss 0.5032050609588623\n",
      "epoch 51 batch 200 loss 0.4918084740638733\n",
      "epoch 52 batch 0 loss 0.5297969579696655\n",
      "epoch 52 batch 100 loss 0.5221992135047913\n",
      "epoch 52 batch 200 loss 0.514601469039917\n",
      "epoch 53 batch 0 loss 0.541193425655365\n",
      "epoch 53 batch 100 loss 0.5373945236206055\n",
      "epoch 53 batch 200 loss 0.4880094528198242\n",
      "epoch 54 batch 0 loss 0.5411933660507202\n",
      "epoch 54 batch 100 loss 0.4880094528198242\n",
      "epoch 54 batch 200 loss 0.49940600991249084\n",
      "epoch 55 batch 0 loss 0.533595621585846\n",
      "epoch 55 batch 100 loss 0.5108025074005127\n",
      "epoch 55 batch 200 loss 0.4956071376800537\n",
      "epoch 56 batch 0 loss 0.5449921488761902\n",
      "epoch 56 batch 100 loss 0.5335955619812012\n",
      "epoch 56 batch 200 loss 0.5146013498306274\n",
      "epoch 57 batch 0 loss 0.5411932468414307\n",
      "epoch 57 batch 100 loss 0.5070036053657532\n",
      "epoch 57 batch 200 loss 0.5146012902259827\n",
      "epoch 58 batch 0 loss 0.5449920892715454\n",
      "epoch 58 batch 100 loss 0.5373943448066711\n",
      "epoch 58 batch 200 loss 0.49940577149391174\n",
      "epoch 59 batch 0 loss 0.5373942852020264\n",
      "epoch 59 batch 100 loss 0.5146011710166931\n",
      "epoch 59 batch 200 loss 0.48800918459892273\n",
      "epoch 60 batch 0 loss 0.5221989154815674\n",
      "epoch 60 batch 100 loss 0.5297965407371521\n",
      "epoch 60 batch 200 loss 0.5032045841217041\n",
      "epoch 61 batch 0 loss 0.5373942852020264\n",
      "epoch 61 batch 100 loss 0.5184000134468079\n",
      "epoch 61 batch 200 loss 0.49940574169158936\n",
      "epoch 62 batch 0 loss 0.5146011114120483\n",
      "epoch 62 batch 100 loss 0.5070034265518188\n",
      "epoch 62 batch 200 loss 0.49940574169158936\n",
      "epoch 63 batch 0 loss 0.5259977579116821\n",
      "epoch 63 batch 100 loss 0.5146011710166931\n",
      "epoch 63 batch 200 loss 0.5184000134468079\n",
      "epoch 64 batch 0 loss 0.5335953831672668\n",
      "epoch 64 batch 100 loss 0.5070034265518188\n",
      "epoch 64 batch 200 loss 0.5032045841217041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 batch 0 loss 0.5335954427719116\n",
      "epoch 65 batch 100 loss 0.5335953831672668\n",
      "epoch 65 batch 200 loss 0.48041149973869324\n",
      "epoch 66 batch 0 loss 0.5373942255973816\n",
      "epoch 66 batch 100 loss 0.5108022689819336\n",
      "epoch 66 batch 200 loss 0.5070034265518188\n",
      "epoch 67 batch 0 loss 0.5525896549224854\n",
      "epoch 67 batch 100 loss 0.49560683965682983\n",
      "epoch 67 batch 200 loss 0.5070033669471741\n",
      "epoch 68 batch 0 loss 0.5525895953178406\n",
      "epoch 68 batch 100 loss 0.4918079376220703\n",
      "epoch 68 batch 200 loss 0.5146011114120483\n",
      "epoch 69 batch 0 loss 0.5032045245170593\n",
      "epoch 69 batch 100 loss 0.4918080270290375\n",
      "epoch 69 batch 200 loss 0.49560683965682983\n",
      "epoch 70 batch 0 loss 0.5411930680274963\n",
      "epoch 70 batch 100 loss 0.5411930680274963\n",
      "epoch 70 batch 200 loss 0.48800909519195557\n",
      "epoch 71 batch 0 loss 0.5259976387023926\n",
      "epoch 71 batch 100 loss 0.5183999538421631\n",
      "epoch 71 batch 200 loss 0.5146010518074036\n",
      "epoch 72 batch 0 loss 0.5259976387023926\n",
      "epoch 72 batch 100 loss 0.48800909519195557\n",
      "epoch 72 batch 200 loss 0.5032045245170593\n",
      "epoch 73 batch 0 loss 0.5297964811325073\n",
      "epoch 73 batch 100 loss 0.5108022093772888\n",
      "epoch 73 batch 200 loss 0.4994056224822998\n",
      "epoch 74 batch 0 loss 0.5297965407371521\n",
      "epoch 74 batch 100 loss 0.5373942255973816\n",
      "epoch 74 batch 200 loss 0.49560683965682983\n",
      "epoch 75 batch 0 loss 0.5335953235626221\n",
      "epoch 75 batch 100 loss 0.5373942255973816\n",
      "epoch 75 batch 200 loss 0.48421019315719604\n",
      "epoch 76 batch 0 loss 0.5297964811325073\n",
      "epoch 76 batch 100 loss 0.4994055926799774\n",
      "epoch 76 batch 200 loss 0.5373941659927368\n",
      "epoch 77 batch 0 loss 0.5449918508529663\n",
      "epoch 77 batch 100 loss 0.5032044649124146\n",
      "epoch 77 batch 200 loss 0.5108022093772888\n",
      "epoch 78 batch 0 loss 0.5183999538421631\n",
      "epoch 78 batch 100 loss 0.5070033073425293\n",
      "epoch 78 batch 200 loss 0.49560678005218506\n",
      "epoch 79 batch 0 loss 0.5183998942375183\n",
      "epoch 79 batch 100 loss 0.5259976387023926\n",
      "epoch 79 batch 200 loss 0.4880090653896332\n",
      "epoch 80 batch 0 loss 0.5411930084228516\n",
      "epoch 80 batch 100 loss 0.5032044649124146\n",
      "epoch 80 batch 200 loss 0.4880090355873108\n",
      "epoch 81 batch 0 loss 0.5297964215278625\n",
      "epoch 81 batch 100 loss 0.5259975790977478\n",
      "epoch 81 batch 200 loss 0.5032044649124146\n",
      "epoch 82 batch 0 loss 0.5411930084228516\n",
      "epoch 82 batch 100 loss 0.5070033073425293\n",
      "epoch 82 batch 200 loss 0.4956067204475403\n",
      "epoch 83 batch 0 loss 0.5373941659927368\n",
      "epoch 83 batch 100 loss 0.5183999538421631\n",
      "epoch 83 batch 200 loss 0.525997519493103\n",
      "epoch 84 batch 0 loss 0.5297964215278625\n",
      "epoch 84 batch 100 loss 0.5032044649124146\n",
      "epoch 84 batch 200 loss 0.4956067204475403\n",
      "epoch 85 batch 0 loss 0.5259976387023926\n",
      "epoch 85 batch 100 loss 0.5335952639579773\n",
      "epoch 85 batch 200 loss 0.5070033073425293\n",
      "epoch 86 batch 0 loss 0.5525895953178406\n",
      "epoch 86 batch 100 loss 0.5335953235626221\n",
      "epoch 86 batch 200 loss 0.5070033073425293\n",
      "epoch 87 batch 0 loss 0.5146010518074036\n",
      "epoch 87 batch 100 loss 0.510802149772644\n",
      "epoch 87 batch 200 loss 0.510802149772644\n",
      "epoch 88 batch 0 loss 0.5146009922027588\n",
      "epoch 88 batch 100 loss 0.510802149772644\n",
      "epoch 88 batch 200 loss 0.4690147042274475\n",
      "epoch 89 batch 0 loss 0.5183998346328735\n",
      "epoch 89 batch 100 loss 0.49180784821510315\n",
      "epoch 89 batch 200 loss 0.5032044053077698\n",
      "epoch 90 batch 0 loss 0.5373941659927368\n",
      "epoch 90 batch 100 loss 0.510802149772644\n",
      "epoch 90 batch 200 loss 0.48421013355255127\n",
      "epoch 91 batch 0 loss 0.5297963619232178\n",
      "epoch 91 batch 100 loss 0.5032044053077698\n",
      "epoch 91 batch 200 loss 0.48421016335487366\n",
      "epoch 92 batch 0 loss 0.5449918508529663\n",
      "epoch 92 batch 100 loss 0.5070033073425293\n",
      "epoch 92 batch 200 loss 0.49940556287765503\n",
      "epoch 93 batch 0 loss 0.5563883781433105\n",
      "epoch 93 batch 100 loss 0.510802149772644\n",
      "epoch 93 batch 200 loss 0.49940556287765503\n",
      "epoch 94 batch 0 loss 0.5525895357131958\n",
      "epoch 94 batch 100 loss 0.5411929488182068\n",
      "epoch 94 batch 200 loss 0.4880090355873108\n",
      "epoch 95 batch 0 loss 0.5259975790977478\n",
      "epoch 95 batch 100 loss 0.49940556287765503\n",
      "epoch 95 batch 200 loss 0.5146009922027588\n",
      "epoch 96 batch 0 loss 0.5032044053077698\n",
      "epoch 96 batch 100 loss 0.5297964215278625\n",
      "epoch 96 batch 200 loss 0.48421019315719604\n",
      "epoch 97 batch 0 loss 0.5297964215278625\n",
      "epoch 97 batch 100 loss 0.5070033073425293\n",
      "epoch 97 batch 200 loss 0.5146009922027588\n",
      "epoch 98 batch 0 loss 0.5449918508529663\n",
      "epoch 98 batch 100 loss 0.5221987366676331\n",
      "epoch 98 batch 200 loss 0.5108020901679993\n",
      "epoch 99 batch 0 loss 0.5259975790977478\n",
      "epoch 99 batch 100 loss 0.5183998346328735\n",
      "epoch 99 batch 200 loss 0.4956067204475403\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    \n",
    "    for i, (train_x, train_y) in enumerate(dataset):\n",
    "        loss = train_step(train_x, train_y)\n",
    "        \n",
    "        if i% 100 == 0:\n",
    "            print(\"epoch {0} batch {1} loss {2}\".format(e, i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3103, shape=(100, 64), dtype=float32, numpy=\n",
       "array([[2.7800e+02, 3.6000e+01, 6.9000e+01, ..., 7.0000e+00, 1.2900e+02,\n",
       "        1.1300e+02],\n",
       "       [1.9400e+02, 1.3519e+04, 1.1697e+04, ..., 1.4000e+01, 1.0700e+02,\n",
       "        1.0200e+02],\n",
       "       [1.4000e+02, 1.4500e+02, 8.0000e+00, ..., 7.0000e+00, 2.5820e+03,\n",
       "        1.0200e+02],\n",
       "       ...,\n",
       "       [3.2100e+02, 8.1100e+02, 5.7328e+04, ..., 3.3630e+03, 1.6860e+03,\n",
       "        4.5100e+02],\n",
       "       [2.5000e+02, 4.7500e+02, 1.1000e+01, ..., 4.0000e+00, 4.2368e+04,\n",
       "        7.0660e+03],\n",
       "       [1.4000e+01, 9.0000e+00, 6.0000e+00, ..., 1.1675e+04, 8.7000e+01,\n",
       "        2.2000e+01]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
