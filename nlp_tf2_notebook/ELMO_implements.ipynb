{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo 向量的重新实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import utils\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(keras.Model):\n",
    "    def __init__(self, v_dim, emb_dim, units, n_layers, lr):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.units = units\n",
    "\n",
    "        # encoder\n",
    "        self.word_embed = keras.layers.Embedding(\n",
    "            input_dim=v_dim, output_dim=emb_dim,  # [n_vocab, emb_dim]\n",
    "            embeddings_initializer=keras.initializers.RandomNormal(0., 0.001),\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        # forward lstm\n",
    "        self.fs = [keras.layers.LSTM(units, return_sequences=True) for _ in range(n_layers)]\n",
    "        self.f_logits = keras.layers.Dense(v_dim)\n",
    "        # backward lstm\n",
    "        self.bs = [keras.layers.LSTM(units, return_sequences=True, go_backwards=True) for _ in range(n_layers)]\n",
    "        self.b_logits = keras.layers.Dense(v_dim)\n",
    "\n",
    "        self.cross_entropy1 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.cross_entropy2 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.opt = keras.optimizers.Adam(lr)\n",
    "\n",
    "    def call(self, seqs):\n",
    "        embedded = self.word_embed(seqs)        # [n, step, dim]\n",
    "        \"\"\"\n",
    "        0123    forward\n",
    "        1234    forward predict\n",
    "         1234   backward \n",
    "         0123   backward predict\n",
    "        \"\"\"\n",
    "        mask = self.word_embed.compute_mask(seqs)\n",
    "        fxs, bxs = [embedded[:, :-1]], [embedded[:, 1:]]\n",
    "        for fl, bl in zip(self.fs, self.bs):\n",
    "            fx = fl(\n",
    "                fxs[-1], mask=mask[:, :-1], initial_state=fl.get_initial_state(fxs[-1])\n",
    "            )           # [n, step-1, dim]\n",
    "            bx = bl(\n",
    "                bxs[-1], mask=mask[:, 1:], initial_state=bl.get_initial_state(bxs[-1])\n",
    "            )  # [n, step-1, dim]\n",
    "            fxs.append(fx)      # predict 1234\n",
    "            bxs.append(tf.reverse(bx, axis=[1]))    # predict 0123\n",
    "        return fxs, bxs\n",
    "\n",
    "    def step(self, seqs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            fxs, bxs = self.call(seqs)\n",
    "            fo, bo = self.f_logits(fxs[-1]), self.b_logits(bxs[-1])\n",
    "            loss = (self.cross_entropy1(seqs[:, 1:], fo) + self.cross_entropy2(seqs[:, :-1], bo))/2\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss, (fo, bo)\n",
    "\n",
    "    def get_emb(self, seqs):\n",
    "        fxs, bxs = self.call(seqs)\n",
    "        xs = [tf.concat((f[:, :-1, :], b[:, 1:, :]), axis=2).numpy() for f, b in zip(fxs, bxs)]\n",
    "        for x in xs:\n",
    "            print(\"layers shape=\", x.shape)\n",
    "        return np.mean(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, step):\n",
    "    t0 = time.time()\n",
    "    for t in range(step):\n",
    "        seqs = data.sample(BATCH_SIZE)\n",
    "        loss, (fo, bo) = model.step(seqs)\n",
    "        if t % 80 == 0:\n",
    "            fp = fo[0].numpy().argmax(axis=1)\n",
    "            bp = bo[0].numpy().argmax(axis=1)\n",
    "            t1 = time.time()\n",
    "            print(\n",
    "                \"\\n\\nstep: \", t,\n",
    "                \"| time: %.2f\" % (t1 - t0),\n",
    "                \"| loss: %.3f\" % loss.numpy(),\n",
    "                \"\\n| tgt: \", \" \".join([data.i2v[i] for i in seqs[0] if i != data.pad_id]),\n",
    "                \"\\n| f_prd: \", \" \".join([data.i2v[i] for i in fp if i != data.pad_id]),\n",
    "                \"\\n| b_prd: \", \" \".join([data.i2v[i] for i in bp if i != data.pad_id]),\n",
    "                )\n",
    "            t0 = t1\n",
    "    os.makedirs(\"./visual/models/elmo\", exist_ok=True)\n",
    "    model.save_weights(\"./visual/models/elmo/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_w2v(model, data):\n",
    "    model.load_weights(\"./visual/models/elmo/model.ckpt\")\n",
    "    emb = model.get_emb(data.sample(4))\n",
    "    print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
