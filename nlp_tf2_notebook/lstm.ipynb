{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm\n",
    "\n",
    "lstm, 全称是 long short term memory neural network, 中文名是长短记忆时网络。 该网络是RNN（Recurent neural network ）的变种。RNN类型相比于全连接神经网络的优势在于，同级别的网络层之间没有连接，那么输出信息只和输入信息相关，和输入的先后顺序无关。同时全连接网络的神经元不能存储信息，那么就没有记忆功能。对于和时间相关的输入数据，全连接网络就不能有效处理。\n",
    "举个例子，\n",
    "如果判别“地球上最高峰是珠穆朗玛峰”这个句子是否表达正确。全连接神经网络会将这个句子划分成几个词（地球，上，最高峰，是，珠穆朗玛峰）， 分布输入到模型中，忽略了词之间的关系。\n",
    "\n",
    "RNN能够有效处理这类数据。然而，RNN自身也存在一些问题。RNN在理论上可以保留所有历史时刻的信息，但是在实际使用中，信息的传递往往会因为间隔太长而逐渐衰减，传递一段时刻以后，信息的作用效果就大大降低了。因此，普通RNN对于信息的长期依赖问题没有很好的处理办法。\n",
    "\n",
    "为了克服这个问题， Hochreiter 等人改进了RNN， 提出了特殊的RNN模型， LSTM, 可以学习到长期依赖信息。\n",
    "\n",
    "\n",
    "lstm网络的结构如下：\n",
    "![title](img/lstm1.png)\n",
    "lstm 基本思想\n",
    "LSTM有以下参数， 输入x_t, 细胞状态C_t, 输出h_t, 遗忘门f, 输入门i, 输出门o。\n",
    "\n",
    "遗忘门f的作用是将细胞状态C_t-1进行评估，0为完全遗忘，1为完全记忆。使用sigmoid函数来处理，将C_t-1 和一个0-1值进行相乘。\n",
    "$$f_t=\\sigma(W_f[h_t-1, x_t]+b_f)$$\n",
    "\n",
    "输入门是针对候选的C_t来进行处理，同样使用sigmid函数进行0-1的映射。\n",
    "$$i_t=\\sigma(W_i[h_t-1, x_t]+b_i)$$\n",
    "\n",
    "输出门是针对C_t 到 h_t的映射，同样使用sigmoid函数\n",
    "\n",
    "$$o_t=\\sigma(W_o[h_t-1, x_t)+b_o)$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A \\mid B) = \\frac{ P(B \\mid A) P(A) }{ P(B) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_target), (test_data, test_target) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_target.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padding = tf.keras.preprocessing.sequence.pad_sequences(train_data, padding=\"post\", maxlen=max_len)\n",
    "test_padding = tf.keras.preprocessing.sequence.pad_sequences(test_data, padding=\"post\", maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padding = train_padding.astype(\"float32\")\n",
    "test_padding = test_padding.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "EMBED_SIZE = 10\n",
    "RNN_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_padding, train_target))\n",
    "dataset = dataset.shuffle(100).batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JLSTM(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(JLSTM, self).__init__()\n",
    "        \n",
    "        self.embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=EMBED_SIZE)\n",
    "        self.rnn = tf.keras.layers.LSTM(RNN_SIZE, return_sequences=True, return_state=True)\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, input_x):\n",
    "        \n",
    "        x = self.embed(input_x)\n",
    "        out, state, _ = self.rnn(x)\n",
    "        out = out[:,-1,:]\n",
    "        logits = self.out(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=232, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.50309056],\n",
       "       [0.50318044]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.constant([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(input_x, input_y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        logits = model(input_x)\n",
    "        loss = loss_func(logits, input_y)\n",
    "        \n",
    "    variables = model.variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss 0.743574321269989\n",
      "epoch 0 batch 100 loss 0.545647144317627\n",
      "epoch 0 batch 200 loss 0.5006479024887085\n",
      "epoch 1 batch 0 loss 0.5313969850540161\n",
      "epoch 1 batch 100 loss 0.5219604969024658\n",
      "epoch 1 batch 200 loss 0.5134150981903076\n",
      "epoch 2 batch 0 loss 0.5318225622177124\n",
      "epoch 2 batch 100 loss 0.5237939357757568\n",
      "epoch 2 batch 200 loss 0.5045776963233948\n",
      "epoch 3 batch 0 loss 0.545950710773468\n",
      "epoch 3 batch 100 loss 0.526868999004364\n",
      "epoch 3 batch 200 loss 0.5267236828804016\n",
      "epoch 4 batch 0 loss 0.5380159616470337\n",
      "epoch 4 batch 100 loss 0.5265653729438782\n",
      "epoch 4 batch 200 loss 0.5075460076332092\n",
      "epoch 5 batch 0 loss 0.5264567732810974\n",
      "epoch 5 batch 100 loss 0.5074487924575806\n",
      "epoch 5 batch 200 loss 0.5149799585342407\n",
      "epoch 6 batch 0 loss 0.5225409865379333\n",
      "epoch 6 batch 100 loss 0.48458412289619446\n",
      "epoch 6 batch 200 loss 0.5262649655342102\n",
      "epoch 7 batch 0 loss 0.530045747756958\n",
      "epoch 7 batch 100 loss 0.5262291431427002\n",
      "epoch 7 batch 200 loss 0.5034438371658325\n",
      "epoch 8 batch 0 loss 0.5489649772644043\n",
      "epoch 8 batch 100 loss 0.5413599610328674\n",
      "epoch 8 batch 200 loss 0.5033961534500122\n",
      "epoch 9 batch 0 loss 0.533750057220459\n",
      "epoch 9 batch 100 loss 0.5223509073257446\n",
      "epoch 9 batch 200 loss 0.476790189743042\n",
      "epoch 10 batch 0 loss 0.5451093912124634\n",
      "epoch 10 batch 100 loss 0.5147315263748169\n",
      "epoch 10 batch 200 loss 0.5109257698059082\n",
      "epoch 11 batch 0 loss 0.5374962687492371\n",
      "epoch 11 batch 100 loss 0.5109129548072815\n",
      "epoch 11 batch 200 loss 0.5147020220756531\n",
      "epoch 12 batch 0 loss 0.5298856496810913\n",
      "epoch 12 batch 100 loss 0.5298795104026794\n",
      "epoch 12 batch 200 loss 0.4957005977630615\n",
      "epoch 13 batch 0 loss 0.5260745882987976\n",
      "epoch 13 batch 100 loss 0.510880708694458\n",
      "epoch 13 batch 200 loss 0.4880915880203247\n",
      "epoch 14 batch 0 loss 0.5032785534858704\n",
      "epoch 14 batch 100 loss 0.5260587930679321\n",
      "epoch 14 batch 200 loss 0.5070672035217285\n",
      "epoch 15 batch 0 loss 0.5260534286499023\n",
      "epoch 15 batch 100 loss 0.5184546709060669\n",
      "epoch 15 batch 200 loss 0.5108557939529419\n",
      "epoch 16 batch 0 loss 0.541236400604248\n",
      "epoch 16 batch 100 loss 0.5032556056976318\n",
      "epoch 16 batch 200 loss 0.503252387046814\n",
      "epoch 17 batch 0 loss 0.5374324321746826\n",
      "epoch 17 batch 100 loss 0.5336323380470276\n",
      "epoch 17 batch 200 loss 0.5070440173149109\n",
      "epoch 18 batch 0 loss 0.5222352147102356\n",
      "epoch 18 batch 100 loss 0.5450217127799988\n",
      "epoch 18 batch 200 loss 0.49944227933883667\n",
      "epoch 19 batch 0 loss 0.5488174557685852\n",
      "epoch 19 batch 100 loss 0.5260266065597534\n",
      "epoch 19 batch 200 loss 0.48804283142089844\n",
      "epoch 20 batch 0 loss 0.5336208939552307\n",
      "epoch 20 batch 100 loss 0.5108296871185303\n",
      "epoch 20 batch 200 loss 0.4994332790374756\n",
      "epoch 21 batch 0 loss 0.5450125336647034\n",
      "epoch 21 batch 100 loss 0.49943116307258606\n",
      "epoch 21 batch 200 loss 0.5222200155258179\n",
      "epoch 22 batch 0 loss 0.5298163294792175\n",
      "epoch 22 batch 100 loss 0.5298152565956116\n",
      "epoch 22 batch 200 loss 0.48423293232917786\n",
      "epoch 23 batch 0 loss 0.5412091612815857\n",
      "epoch 23 batch 100 loss 0.5374098420143127\n",
      "epoch 23 batch 200 loss 0.5222150683403015\n",
      "epoch 24 batch 0 loss 0.5412071347236633\n",
      "epoch 24 batch 100 loss 0.5032211542129517\n",
      "epoch 24 batch 200 loss 0.4956231713294983\n",
      "epoch 25 batch 0 loss 0.5563995242118835\n",
      "epoch 25 batch 100 loss 0.5184133052825928\n",
      "epoch 25 batch 200 loss 0.5260097980499268\n",
      "epoch 26 batch 0 loss 0.5222108960151672\n",
      "epoch 26 batch 100 loss 0.49562013149261475\n",
      "epoch 26 batch 200 loss 0.51081383228302\n",
      "epoch 27 batch 0 loss 0.5450011491775513\n",
      "epoch 27 batch 100 loss 0.5146116018295288\n",
      "epoch 27 batch 200 loss 0.4956178665161133\n",
      "epoch 28 batch 0 loss 0.5146108865737915\n",
      "epoch 28 batch 100 loss 0.49941569566726685\n",
      "epoch 28 batch 200 loss 0.5032138228416443\n",
      "epoch 29 batch 0 loss 0.5336029529571533\n",
      "epoch 29 batch 100 loss 0.49561581015586853\n",
      "epoch 29 batch 200 loss 0.5032127499580383\n",
      "epoch 30 batch 0 loss 0.5374007225036621\n",
      "epoch 30 batch 100 loss 0.5260043144226074\n",
      "epoch 30 batch 200 loss 0.5108091235160828\n",
      "epoch 31 batch 0 loss 0.5336012840270996\n",
      "epoch 31 batch 100 loss 0.5222048163414001\n",
      "epoch 31 batch 200 loss 0.5032108426094055\n",
      "epoch 32 batch 0 loss 0.5146068334579468\n",
      "epoch 32 batch 100 loss 0.5411977171897888\n",
      "epoch 32 batch 200 loss 0.5260025262832642\n",
      "epoch 33 batch 0 loss 0.5335999131202698\n",
      "epoch 33 batch 100 loss 0.4842159152030945\n",
      "epoch 33 batch 200 loss 0.5373982191085815\n",
      "epoch 34 batch 0 loss 0.5260018706321716\n",
      "epoch 34 batch 100 loss 0.5108065605163574\n",
      "epoch 34 batch 200 loss 0.4804161489009857\n",
      "epoch 35 batch 0 loss 0.5222025513648987\n",
      "epoch 35 batch 100 loss 0.5260011553764343\n",
      "epoch 35 batch 200 loss 0.5260009169578552\n",
      "epoch 36 batch 0 loss 0.5335984230041504\n",
      "epoch 36 batch 100 loss 0.510805606842041\n",
      "epoch 36 batch 200 loss 0.5108054280281067\n",
      "epoch 37 batch 0 loss 0.5297992825508118\n",
      "epoch 37 batch 100 loss 0.49560999870300293\n",
      "epoch 37 batch 200 loss 0.49940863251686096\n",
      "epoch 38 batch 0 loss 0.5601893663406372\n",
      "epoch 38 batch 100 loss 0.5222011804580688\n",
      "epoch 38 batch 200 loss 0.5184023380279541\n",
      "epoch 39 batch 0 loss 0.533597469329834\n",
      "epoch 39 batch 100 loss 0.49181050062179565\n",
      "epoch 39 batch 200 loss 0.503206729888916\n",
      "epoch 40 batch 0 loss 0.5373960137367249\n",
      "epoch 40 batch 100 loss 0.5184018611907959\n",
      "epoch 40 batch 200 loss 0.5222005248069763\n",
      "epoch 41 batch 0 loss 0.55259108543396\n",
      "epoch 41 batch 100 loss 0.5184016227722168\n",
      "epoch 41 batch 200 loss 0.5108038783073425\n",
      "epoch 42 batch 0 loss 0.5449932217597961\n",
      "epoch 42 batch 100 loss 0.4842120409011841\n",
      "epoch 42 batch 200 loss 0.5070048570632935\n",
      "epoch 43 batch 0 loss 0.5373954176902771\n",
      "epoch 43 batch 100 loss 0.5411942005157471\n",
      "epoch 43 batch 200 loss 0.5221999883651733\n",
      "epoch 44 batch 0 loss 0.5563893914222717\n",
      "epoch 44 batch 100 loss 0.533596396446228\n",
      "epoch 44 batch 200 loss 0.5032056570053101\n",
      "epoch 45 batch 0 loss 0.541193962097168\n",
      "epoch 45 batch 100 loss 0.5221997499465942\n",
      "epoch 45 batch 200 loss 0.4956078827381134\n",
      "epoch 46 batch 0 loss 0.51460200548172\n",
      "epoch 46 batch 100 loss 0.48801013827323914\n",
      "epoch 46 batch 200 loss 0.5221995711326599\n",
      "epoch 47 batch 0 loss 0.5449925661087036\n",
      "epoch 47 batch 100 loss 0.5146017670631409\n",
      "epoch 47 batch 200 loss 0.4956076443195343\n",
      "epoch 48 batch 0 loss 0.5259983539581299\n",
      "epoch 48 batch 100 loss 0.5259982943534851\n",
      "epoch 48 batch 200 loss 0.5184005498886108\n",
      "epoch 49 batch 0 loss 0.5373947620391846\n",
      "epoch 49 batch 100 loss 0.533595860004425\n",
      "epoch 49 batch 200 loss 0.507003903388977\n",
      "epoch 50 batch 0 loss 0.5335958003997803\n",
      "epoch 50 batch 100 loss 0.5259981155395508\n",
      "epoch 50 batch 200 loss 0.48800963163375854\n",
      "epoch 51 batch 0 loss 0.5449922680854797\n",
      "epoch 51 batch 100 loss 0.5221992135047913\n",
      "epoch 51 batch 200 loss 0.5070038437843323\n",
      "epoch 52 batch 0 loss 0.529796838760376\n",
      "epoch 52 batch 100 loss 0.514601469039917\n",
      "epoch 52 batch 200 loss 0.4956071972846985\n",
      "epoch 53 batch 0 loss 0.544992208480835\n",
      "epoch 53 batch 100 loss 0.49940600991249084\n",
      "epoch 53 batch 200 loss 0.4880094528198242\n",
      "epoch 54 batch 0 loss 0.5373944640159607\n",
      "epoch 54 batch 100 loss 0.5108025074005127\n",
      "epoch 54 batch 200 loss 0.4918082356452942\n",
      "epoch 55 batch 0 loss 0.5221990346908569\n",
      "epoch 55 batch 100 loss 0.5221990346908569\n",
      "epoch 55 batch 200 loss 0.4804116487503052\n",
      "epoch 56 batch 0 loss 0.5411932468414307\n",
      "epoch 56 batch 100 loss 0.5032047629356384\n",
      "epoch 56 batch 200 loss 0.5221989750862122\n",
      "epoch 57 batch 0 loss 0.5221989154815674\n",
      "epoch 57 batch 100 loss 0.5221989154815674\n",
      "epoch 57 batch 200 loss 0.5108023285865784\n",
      "epoch 58 batch 0 loss 0.5297966003417969\n",
      "epoch 58 batch 100 loss 0.5297966003417969\n",
      "epoch 58 batch 200 loss 0.480411559343338\n",
      "epoch 59 batch 0 loss 0.5221989154815674\n",
      "epoch 59 batch 100 loss 0.49180805683135986\n",
      "epoch 59 batch 200 loss 0.4880092740058899\n",
      "epoch 60 batch 0 loss 0.5373942852020264\n",
      "epoch 60 batch 100 loss 0.5108023285865784\n",
      "epoch 60 batch 200 loss 0.48800918459892273\n",
      "epoch 61 batch 0 loss 0.5487908124923706\n",
      "epoch 61 batch 100 loss 0.5108023285865784\n",
      "epoch 61 batch 200 loss 0.5259976983070374\n",
      "epoch 62 batch 0 loss 0.5411930680274963\n",
      "epoch 62 batch 100 loss 0.5221988558769226\n",
      "epoch 62 batch 200 loss 0.4842103123664856\n",
      "epoch 63 batch 0 loss 0.5487908124923706\n",
      "epoch 63 batch 100 loss 0.4918080270290375\n",
      "epoch 63 batch 200 loss 0.5108023285865784\n",
      "epoch 64 batch 0 loss 0.5373942852020264\n",
      "epoch 64 batch 100 loss 0.5259977579116821\n",
      "epoch 64 batch 200 loss 0.48800915479660034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 batch 0 loss 0.5259976387023926\n",
      "epoch 65 batch 100 loss 0.5335954427719116\n",
      "epoch 65 batch 200 loss 0.5070034265518188\n",
      "epoch 66 batch 0 loss 0.5411931276321411\n",
      "epoch 66 batch 100 loss 0.5108022689819336\n",
      "epoch 66 batch 200 loss 0.49940571188926697\n",
      "epoch 67 batch 0 loss 0.5259976983070374\n",
      "epoch 67 batch 100 loss 0.5297964811325073\n",
      "epoch 67 batch 200 loss 0.49940571188926697\n",
      "epoch 68 batch 0 loss 0.5411930680274963\n",
      "epoch 68 batch 100 loss 0.5411930680274963\n",
      "epoch 68 batch 200 loss 0.48800909519195557\n",
      "epoch 69 batch 0 loss 0.5032045245170593\n",
      "epoch 69 batch 100 loss 0.5297965407371521\n",
      "epoch 69 batch 200 loss 0.49560683965682983\n",
      "epoch 70 batch 0 loss 0.5563884973526001\n",
      "epoch 70 batch 100 loss 0.5183999538421631\n",
      "epoch 70 batch 200 loss 0.49560680985450745\n",
      "epoch 71 batch 0 loss 0.5411930680274963\n",
      "epoch 71 batch 100 loss 0.4918079376220703\n",
      "epoch 71 batch 200 loss 0.5146010518074036\n",
      "epoch 72 batch 0 loss 0.5297964811325073\n",
      "epoch 72 batch 100 loss 0.49560678005218506\n",
      "epoch 72 batch 200 loss 0.5487907528877258\n",
      "epoch 73 batch 0 loss 0.5449919104576111\n",
      "epoch 73 batch 100 loss 0.5221987366676331\n",
      "epoch 73 batch 200 loss 0.5146010518074036\n",
      "epoch 74 batch 0 loss 0.5335953235626221\n",
      "epoch 74 batch 100 loss 0.5335953235626221\n",
      "epoch 74 batch 200 loss 0.4728136360645294\n",
      "epoch 75 batch 0 loss 0.5335953235626221\n",
      "epoch 75 batch 100 loss 0.5032045245170593\n",
      "epoch 75 batch 200 loss 0.5108022093772888\n",
      "epoch 76 batch 0 loss 0.5183998942375183\n",
      "epoch 76 batch 100 loss 0.5183998942375183\n",
      "epoch 76 batch 200 loss 0.5183998942375183\n",
      "epoch 77 batch 0 loss 0.5335952639579773\n",
      "epoch 77 batch 100 loss 0.4690147340297699\n",
      "epoch 77 batch 200 loss 0.49560678005218506\n",
      "epoch 78 batch 0 loss 0.5335952639579773\n",
      "epoch 78 batch 100 loss 0.5032044649124146\n",
      "epoch 78 batch 200 loss 0.4918079078197479\n",
      "epoch 79 batch 0 loss 0.5146009922027588\n",
      "epoch 79 batch 100 loss 0.5070033073425293\n",
      "epoch 79 batch 200 loss 0.5259975790977478\n",
      "epoch 80 batch 0 loss 0.5411930084228516\n",
      "epoch 80 batch 100 loss 0.5070033073425293\n",
      "epoch 80 batch 200 loss 0.48421019315719604\n",
      "epoch 81 batch 0 loss 0.537394106388092\n",
      "epoch 81 batch 100 loss 0.5183998942375183\n",
      "epoch 81 batch 200 loss 0.4994055926799774\n",
      "epoch 82 batch 0 loss 0.5449918508529663\n",
      "epoch 82 batch 100 loss 0.5032044053077698\n",
      "epoch 82 batch 200 loss 0.4728136360645294\n",
      "epoch 83 batch 0 loss 0.5297964215278625\n",
      "epoch 83 batch 100 loss 0.4880090057849884\n",
      "epoch 83 batch 200 loss 0.49560678005218506\n",
      "epoch 84 batch 0 loss 0.5259975790977478\n",
      "epoch 84 batch 100 loss 0.47661247849464417\n",
      "epoch 84 batch 200 loss 0.5146009922027588\n",
      "epoch 85 batch 0 loss 0.5449918508529663\n",
      "epoch 85 batch 100 loss 0.5221987366676331\n",
      "epoch 85 batch 200 loss 0.5070032477378845\n",
      "epoch 86 batch 0 loss 0.5373941659927368\n",
      "epoch 86 batch 100 loss 0.4918079078197479\n",
      "epoch 86 batch 200 loss 0.5221987366676331\n",
      "epoch 87 batch 0 loss 0.5525895953178406\n",
      "epoch 87 batch 100 loss 0.5563883781433105\n",
      "epoch 87 batch 200 loss 0.48421013355255127\n",
      "epoch 88 batch 0 loss 0.5297963619232178\n",
      "epoch 88 batch 100 loss 0.5108020901679993\n",
      "epoch 88 batch 200 loss 0.4956067204475403\n",
      "epoch 89 batch 0 loss 0.5449917912483215\n",
      "epoch 89 batch 100 loss 0.5411929488182068\n",
      "epoch 89 batch 200 loss 0.5335952639579773\n",
      "epoch 90 batch 0 loss 0.5297964215278625\n",
      "epoch 90 batch 100 loss 0.4994055926799774\n",
      "epoch 90 batch 200 loss 0.4880090057849884\n",
      "epoch 91 batch 0 loss 0.5411929488182068\n",
      "epoch 91 batch 100 loss 0.4918079078197479\n",
      "epoch 91 batch 200 loss 0.4994055926799774\n",
      "epoch 92 batch 0 loss 0.5373941659927368\n",
      "epoch 92 batch 100 loss 0.5221987366676331\n",
      "epoch 92 batch 200 loss 0.49940556287765503\n",
      "epoch 93 batch 0 loss 0.5449918508529663\n",
      "epoch 93 batch 100 loss 0.5108020901679993\n",
      "epoch 93 batch 200 loss 0.5183998346328735\n",
      "epoch 94 batch 0 loss 0.5411929488182068\n",
      "epoch 94 batch 100 loss 0.5070032477378845\n",
      "epoch 94 batch 200 loss 0.525997519493103\n",
      "epoch 95 batch 0 loss 0.5297963619232178\n",
      "epoch 95 batch 100 loss 0.525997519493103\n",
      "epoch 95 batch 200 loss 0.5108020901679993\n",
      "epoch 96 batch 0 loss 0.5146009922027588\n",
      "epoch 96 batch 100 loss 0.5221987366676331\n",
      "epoch 96 batch 200 loss 0.49180781841278076\n",
      "epoch 97 batch 0 loss 0.5449918508529663\n",
      "epoch 97 batch 100 loss 0.49940556287765503\n",
      "epoch 97 batch 200 loss 0.4804112911224365\n",
      "epoch 98 batch 0 loss 0.5563883781433105\n",
      "epoch 98 batch 100 loss 0.5297963619232178\n",
      "epoch 98 batch 200 loss 0.4918079078197479\n",
      "epoch 99 batch 0 loss 0.5259976387023926\n",
      "epoch 99 batch 100 loss 0.5221987366676331\n",
      "epoch 99 batch 200 loss 0.4994055926799774\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    \n",
    "    for i, (train_x, train_y) in enumerate(dataset):\n",
    "        loss = train_step(train_x, train_y)\n",
    "        \n",
    "        if i% 100 == 0:\n",
    "            print(\"epoch {0} batch {1} loss {2}\".format(e, i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
