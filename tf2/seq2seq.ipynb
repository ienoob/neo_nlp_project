{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 模型最开始应用于机器翻译。\n",
    "\n",
    "机器翻译\n",
    "早期的机器翻译的思路十分简单，通过设置大量的翻译规则，构建一个大型的双语对照表，来将源语言翻译成目标语言。\n",
    "\n",
    "后来1990-2010s, 研究者梦开发了更先进复杂的机器翻译技术，（统计机器翻译）Statistical Machine Translation, SMT.的主要原理是从大量的数据中学习一个概率模型p(y|x), 其中x是源语言，y是目标语言。翻译的时候，需要通过求argmaxyP(y|x) 就行\n",
    "\n",
    "y和x 都是一个句子。\n",
    "\n",
    "$$P(y|x)=\\frac{P(y)P(x|y)}{P(x)}$$\n",
    "\n",
    "那么\n",
    "$$argmax_yP(y|x)=argmax_y\\frac{P(y)P(x|y)}{P(x)}=argmax_yP(y)P(x|y)$$\n",
    "\n",
    "P(y)是一个语言模型，而P(x|y)则被称为翻译模型。LM通过目标语料库进行训练，而TM通过平行语料进行训练。\n",
    "\n",
    "在学习完LM和TM这两个模型之后，需要使用模型，即寻找最佳翻译，y的过程。这个过程也被称为decoding\n",
    "\n",
    "decoding的做法是通过CRF或HMM的解码。统计机器翻译，虽然比较强但是存在一些问题\n",
    "- SMT的门槛比较高，表现比较好的模型，通常比较复杂\n",
    "- SMT模型需要庞大的人力去维护\n",
    "\n",
    "神经机器翻译（NMT）\n",
    "NMT的优势在于将大量的人工操作进行省略，下面是SMT和NMT的对比\n",
    "![title](img/SMT-vs-NMT.png)\n",
    "NMT 使用网络结构是sequence-to-sequence, 即seq2seq。 同时也被称为encoder-decoder结构。\n",
    "\n",
    "\n",
    "![title](img/seq2seq_1.png)\n",
    "根据图中结构，seq2seq中的Encoder读取输入文本，也就是源语言文本。encoder的作用是将输入文本转化成向量，Decoder的作用是将向量转化为输出文本，即目标语言文本。Encoder的行为是编码，而Decoder的行为是解码。\n",
    "\n",
    "对于其中的具体结构，如下所示\n",
    "\n",
    "在Encoder短，将source通过Embedding层，将文本转化为词向量，然后经过一个RNN\\LSTM\\GRU等RNN类型网络层，输出背景向量（context vector）\n",
    "\n",
    "在Decoder端， 输入的是背景向量以及目标语言的词向量。目标语言的第一个输入是<start>, 每一步根据当前正确的输入词以及上一步的隐状态来预处下一步的输出词。\n",
    "    \n",
    "   \n",
    " 对于预测的时候，Encoder端和训练时相同，而Decoder端部分需要Context vector 以及 初始输入\"<start>\"。然后后面的RNN结构的网络层的每一次的输入是上一个单元的隐藏层，以及预测的本次的单词的词向量。直到预测的单词为\"<end>\"或者序列长度达到指定值为止。\n",
    " \n",
    "decoder在训练的时候和预测的时候，其流程有所不同，这两个模式有专门的名词。根据标准答案也就训练的方式，称为（teacher forcing）, 根据上一步的输出作为下一步输入的decode方式为（free running）. 也就是预测的时候用。\n",
    "    \n",
    "free running 在训练的时候可以用，这在理论上没问题，可是，在实践中，由于没有指导，那就会导致误差爆炸（bias exposure）, 就是说前面一步错了，后面错的可能性就会更高，所谓一步错，步步错。但是如果在每一步预测的时候，给予相应的指导，那么decoder就可以让训练更快收敛。\n",
    "    \n",
    "然而，teacher forcing的方式同样存在一些问题，就是预测时，没有了指导（标记），那么同样有可能会偏离正确的道路。\n",
    "    \n",
    " 比较好的思路，就将两种方法进行结合，也就是，既有指导，也有自由发挥。具体的实现方式，就是我们设置一个概率p, 随机一个0-1的值。如果大于p，那么就使用靠上一步的输入来预测，反之，则使用指导来进行预测，这种方法称为计划采样（scheduled sampling）\n",
    "  \n",
    "## seq2seq 的损失函数\n",
    "    \n",
    " $$J=-log(p(\\widehat{y_1}))-log(p(\\widehat{y_2}))-...-log(p([EOS]))=\\frac{1}{T}\\sum_{i}^{T}{log(p(\\widehat(y_i)))}$$\n",
    " 其中T代表Decode有多少步，[EOS] 表示 end of sentence. \n",
    "    \n",
    "## Decoding 和Bean Search\n",
    "    \n",
    "对于最后预测结果的产生，有很多种方式，包括greedy算法、全局搜索以及beam 搜索。全局搜索的计算复杂度指数型的，greedy算法虽然是线性的，但是很难达到全局最优，甚至次优都可能达不到。beam搜索的方法相对于greedy方法，产生候选集，然后基于候选集选出最优结果。\n",
    "    \n",
    "- 首先设定一个候选集大小为k\n",
    "- 每一步选择可能性最大的k个路径作为下一步的候选集\n",
    "- 直到最后结束，从k个候选集中选择可能性最大的路径。\n",
    "\n",
    " \n",
    " NMT的优缺点\n",
    " 优点是不需要构建人工特征，置需构建端到端的网络\n",
    " 不足\n",
    " \n",
    " 缺点：\n",
    " NMT的解释性差，难以调试，难以控制\n",
    "    \n",
    "    \n",
    " 文献\n",
    " 【1】CS224n笔记[7 ]:整理了12小时，只为让你20分钟搞懂Seq2seq https://zhuanlan.zhihu.com/p/147310766\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cmn.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "en = []\n",
    "cn = []\n",
    "for x in data.split(\"\\n\"):\n",
    "    if len(x.split(\"\\t\")) < 2:\n",
    "        continue\n",
    "    ei, ci = x.split(\"\\t\")\n",
    "    ei = ei.lower()\n",
    "    ei = re.sub(r\"([.?,])\", r\" \\1\", ei)\n",
    "    \n",
    "    en.append(re.split(r\"\\s\", ei))\n",
    "    cn.append([c for c in ci])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2id = {\"<start>\": 1, \"<pad>\": 0}\n",
    "cn_word2id = {\"<end>\": 1, \"<start>\": 2, \"<pad>\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en = []\n",
    "for ei in en:\n",
    "    input_en.append(ei)\n",
    "    \n",
    "    for e in ei:\n",
    "        if e not in en_word2id:\n",
    "            en_word2id[e] = len(en_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cn = []\n",
    "target_cn = []\n",
    "for ci in cn:\n",
    "    input_cn.append([\"<start>\"]+ci)\n",
    "    target_cn.append(ci+[\"<end>\"])\n",
    "    \n",
    "    for c in ci:\n",
    "        if c not in cn_word2id:\n",
    "            cn_word2id[c] = len(cn_word2id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en_id = [[en_word2id[e] for e in ei] for ei in input_en]\n",
    "input_cn_id = [[cn_word2id[c] for c in ci] for ci in input_cn]\n",
    "target_cn_id = [[cn_word2id[c] for c in ci] for ci in target_cn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en_id_pad = tf.keras.preprocessing.sequence.pad_sequences(input_en_id, maxlen=64, padding=\"post\")\n",
    "input_cn_id_pad = tf.keras.preprocessing.sequence.pad_sequences(input_cn_id, maxlen=64, padding=\"post\")\n",
    "target_cn_id_pad = tf.keras.preprocessing.sequence.pad_sequences(target_cn_id, maxlen=64, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_en_id_pad, input_cn_id_pad, target_cn_id_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(100).batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'ran', '']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"[\\s]\", en[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDING_SIZE = 10\n",
    "EN_VOCAB_SIZE = len(en_word2id)\n",
    "CN_VOCAB_SIZE = len(cn_word2id)\n",
    "\n",
    "VOCAB_SIZE = 10\n",
    "LSTM_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embed = tf.keras.layers.Embedding(EN_VOCAB_SIZE, EMBEDING_SIZE)\n",
    "        self.lstm = tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=False, return_state=True)\n",
    "        \n",
    "    def call(self, input_x):\n",
    "        x = self.word_embed(input_x)\n",
    "        \n",
    "        x, h_state, c_state = self.lstm(x)\n",
    "        \n",
    "        return x, h_state, c_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embed = tf.keras.layers.Embedding(CN_VOCAB_SIZE, EMBEDING_SIZE)\n",
    "        self.lstm = tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=True, return_state=True)\n",
    "        self.out = tf.keras.layers.Dense(CN_VOCAB_SIZE, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, input_x, input_state):\n",
    "        x = self.word_embed(input_x)\n",
    "        output, h_state, c_state = self.lstm(x, initial_state=input_state)\n",
    "        \n",
    "        logits = self.out(output)\n",
    "        \n",
    "        return logits, h_state, c_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y, z) = encoder(tf.constant([[1, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = decoder(tf.constant([[1, 2]]), [y, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 10])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4800, shape=(), dtype=float32, numpy=8.144191>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fun(tf.constant([[1, 1]]), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_value(true_val, pred_val):\n",
    "    return loss_func(true_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_x, input_y, target_y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "    \n",
    "        _, h_state, c_state = encoder(input_x)\n",
    "    \n",
    "        out, h, c = decoder(input_y, [h_state, c_state])\n",
    "        \n",
    "        loss = loss_fun(target_y, out)\n",
    "        \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss 8.144418716430664\n",
      "epoch 0 batch 100 loss 8.134079933166504\n",
      "epoch 0 batch 200 loss 7.446378231048584\n",
      "epoch 1 batch 0 loss 7.288571834564209\n",
      "epoch 1 batch 100 loss 7.328464508056641\n",
      "epoch 1 batch 200 loss 7.39698600769043\n",
      "epoch 2 batch 0 loss 7.240288257598877\n",
      "epoch 2 batch 100 loss 7.314799785614014\n",
      "epoch 2 batch 200 loss 7.390826225280762\n",
      "epoch 3 batch 0 loss 7.233001232147217\n",
      "epoch 3 batch 100 loss 7.311596870422363\n",
      "epoch 3 batch 200 loss 7.390583515167236\n",
      "epoch 4 batch 0 loss 7.229872226715088\n",
      "epoch 4 batch 100 loss 7.302859306335449\n",
      "epoch 4 batch 200 loss 7.384603977203369\n",
      "epoch 5 batch 0 loss 7.311896800994873\n",
      "epoch 5 batch 100 loss 7.298801422119141\n",
      "epoch 5 batch 200 loss 7.391597747802734\n",
      "epoch 6 batch 0 loss 7.242038726806641\n",
      "epoch 6 batch 100 loss 7.298707485198975\n",
      "epoch 6 batch 200 loss 7.3855414390563965\n",
      "epoch 7 batch 0 loss 7.250176429748535\n",
      "epoch 7 batch 100 loss 7.297691822052002\n",
      "epoch 7 batch 200 loss 7.382099628448486\n",
      "epoch 8 batch 0 loss 7.233062744140625\n",
      "epoch 8 batch 100 loss 7.298477649688721\n",
      "epoch 8 batch 200 loss 7.383493423461914\n",
      "epoch 9 batch 0 loss 7.225228786468506\n",
      "epoch 9 batch 100 loss 7.295296669006348\n",
      "epoch 9 batch 200 loss 7.386164665222168\n",
      "epoch 10 batch 0 loss 7.22730827331543\n",
      "epoch 10 batch 100 loss 7.295753002166748\n",
      "epoch 10 batch 200 loss 7.384869575500488\n",
      "epoch 11 batch 0 loss 7.225894927978516\n",
      "epoch 11 batch 100 loss 7.296605110168457\n",
      "epoch 11 batch 200 loss 7.382340908050537\n",
      "epoch 12 batch 0 loss 7.223462104797363\n",
      "epoch 12 batch 100 loss 7.295495510101318\n",
      "epoch 12 batch 200 loss 7.3817315101623535\n",
      "epoch 13 batch 0 loss 7.224025249481201\n",
      "epoch 13 batch 100 loss 7.296735763549805\n",
      "epoch 13 batch 200 loss 7.383726596832275\n",
      "epoch 14 batch 0 loss 7.223156929016113\n",
      "epoch 14 batch 100 loss 7.297509670257568\n",
      "epoch 14 batch 200 loss 7.386254787445068\n",
      "epoch 15 batch 0 loss 7.223968982696533\n",
      "epoch 15 batch 100 loss 7.296724319458008\n",
      "epoch 15 batch 200 loss 7.379070281982422\n",
      "epoch 16 batch 0 loss 7.223817348480225\n",
      "epoch 16 batch 100 loss 7.298593521118164\n",
      "epoch 16 batch 200 loss 7.3829450607299805\n",
      "epoch 17 batch 0 loss 7.222250938415527\n",
      "epoch 17 batch 100 loss 7.295140266418457\n",
      "epoch 17 batch 200 loss 7.382326602935791\n",
      "epoch 18 batch 0 loss 7.225335597991943\n",
      "epoch 18 batch 100 loss 7.2964301109313965\n",
      "epoch 18 batch 200 loss 7.3861894607543945\n",
      "epoch 19 batch 0 loss 7.221661567687988\n",
      "epoch 19 batch 100 loss 7.30085563659668\n",
      "epoch 19 batch 200 loss 7.380086898803711\n",
      "epoch 20 batch 0 loss 7.223682403564453\n",
      "epoch 20 batch 100 loss 7.2953782081604\n",
      "epoch 20 batch 200 loss 7.3805413246154785\n",
      "epoch 21 batch 0 loss 7.221176624298096\n",
      "epoch 21 batch 100 loss 7.294438362121582\n",
      "epoch 21 batch 200 loss 7.385537147521973\n",
      "epoch 22 batch 0 loss 7.223670482635498\n",
      "epoch 22 batch 100 loss 7.295997142791748\n",
      "epoch 22 batch 200 loss 7.3833513259887695\n",
      "epoch 23 batch 0 loss 7.2241339683532715\n",
      "epoch 23 batch 100 loss 7.2985005378723145\n",
      "epoch 23 batch 200 loss 7.383974075317383\n",
      "epoch 24 batch 0 loss 7.22366189956665\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    \n",
    "    for batch, (input_x, input_y, target_y) in enumerate(dataset):\n",
    "        \n",
    "        loss = train_step(input_x, input_y, target_y)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(\"epoch {0} batch {1} loss {2}\".format(i, batch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_en(input_s):\n",
    "    input_s = input_s.lower()\n",
    "    input_s = re.sub(r\"([.?,])\", r\" \\1\", input_s)\n",
    "    input_s = re.split(\"\\s\", input_s)\n",
    "    \n",
    "    input_s_id = []\n",
    "    for s in input_s:\n",
    "        input_s_id.append(en_word2id[s])\n",
    "    \n",
    "    for i in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_x: str):\n",
    "    \n",
    "    input_x_id = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
