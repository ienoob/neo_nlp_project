{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding\n",
    "词向量模型，主要功能是用于表示\n",
    "\n",
    "参考\n",
    "- cs224n 第二讲和第三讲\n",
    "- speech and language processing chapter 16 -- semantic with dense vectors\n",
    "- https://www.jiqizhixin.com/articles/2020-06-30-11 推荐系统 embedding 技术实践总结\n",
    "- 刘建平 word2vec原理(三) 基于Negative Sampling的模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本表征的方法\n",
    "文本表征的目的是将文本转化成机器学习算法可以处理的数字形式，换句话说就是将文本使用数学语言进行表示。\n",
    "- 词袋模型 one-hot\\tf-idf\n",
    "- 主题模型，LSA\\PLSA\\LDA等\n",
    "- 词向量模型 word2vec fastText glove\n",
    "- 动态词向量模型 elmo BERT GPT\n",
    "\n",
    "词向量模型将一个词表示成一个先对要短（1000以内的长度）以及稠密的向量（大部分值不为0）\n",
    "词袋模型简单，但是存在维数灾难和语义鸿沟的问题\n",
    "主题模型训练计算复杂度高\n",
    "早期的词向量的研究源于语言模型，比附NNLM和RNNLM, 词向量是副产物。\n",
    "**语言模型，就是用来计算一个词序列的概率的模型。**\n",
    "\n",
    "\n",
    "分布式假设：相同上下文语境的词具有相似含义，并且由此引出了word2vec和fastText等。这类词向量中，虽然其本质仍然是语言模型，但其目标是生成词向量。\n",
    "\n",
    "embdding技术尽管非常实用，但是依然存在一些问题。【3】比如\n",
    "- 增量更新前后语义不变\n",
    "- cover多个维度的特征\n",
    "- 多模态向量融合\n",
    "- 长尾数据资料少难以充分训练\n",
    "- Embedding空间分布，影响模型泛化误差\n",
    "\n",
    "fastText 相比于word2vec, 训练词向量会考虑subword。还可以进行文本分类\n",
    "\n",
    "glove 的特点\n",
    "glove相比于LSA， 采用Adagrad 对最小平方损失进行优化，可以算是一种经过优化的矩阵分解算法。\n",
    "\n",
    "glove 相比于word2vec， 使用了全局语料，同时glove不能进行在线学习，因为其需要统计固定语料信息\n",
    "\n",
    "glove的损失韩式最小平方损失函数，相比于word2vec的带权重的交叉熵，\n",
    "\n",
    "\n",
    "elmo、GPT和bert \n",
    "这三个均为动态词向量。动态词向量的特点是在不同上下文环境下，词向量也会不相同。这就解决了一词多义问题。\n",
    "\n",
    "elmo使用LSTM进行提取特征，使用双向语言模型\n",
    "GPT和bert使用transformer来提取特征\n",
    "bert使用双向语言模型\n",
    "\n",
    "word2vec 有两种实现方式, CBOW 和 Skip-gram\n",
    "\n",
    "CBOW 使用上下文背景词来预测目标词\n",
    "Skip-gram使用目标词来预测背景词\n",
    "\n",
    "\n",
    "为了加快word2vec的训练速度，使用了negative sampling 和 Hierarchical softmax.\n",
    "\n",
    "\n",
    "\n",
    "如果使用softmax 那么输出时间复杂度为O(N). 如果词的数量越多，那么就会花费更长时间来做输出。同时有这么多的输出，其概率分布很可能会比较均衡，难以导致模型的收敛。\n",
    "hierarchical softmax 的核心想法，是将输出形式变为二叉树的形式。\n",
    "\n",
    "negative sampling 使用采样的方法。对于一个训练样本，存在一个中心词w以及上下背景词2c个， 记为context(w)。中心词w和背景词具有相关关系，可以认定其为正例，通过negative sampling采样，我们可以得到neg个和w不同的中心词wi-neg. 这样context(w)和w_i组成了neg个并不真实存在负例。我们基于一个正例和neg个负例，来进行二元逻辑回归。最后得到负采样模型参数以及每个词的词向量。\n",
    "\n",
    "存在两个问题\n",
    "1）如何进行负采样？\n",
    "2）如何通过一个正例和neg个负例进行二元逻辑回归。\n",
    "\n",
    "如何进行负采样\n",
    "首先我们可以收集到所有的词汇，并统计每个词汇的词频。这样我们就可以得到每个词汇的长度，其长度和词汇的词频有关。词频越高，则长度越长。\n",
    "\n",
    "\n",
    "\n",
    "在word2vec 中，分子和分母都取3/4次幂\n",
    "\n",
    "在采样前，将长度为1的现端划分为M等分，这里M>>V。这样可以保证每个词对应的线段都会划分成对应的小块。而M份的\n",
    "每一份都会落在某一个词对应的线段商。在采样的时候\n",
    "只需要从M个位置中采样出neg个位置就行。\n",
    "\n",
    "在word2vec中，M取值默认为10^8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gutenberg.words(\"austen-emma.txt\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "for d in data:\n",
    "    d = d.lower()\n",
    "    word_count[d] = word_count.get(d, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [(k, v) for k, v in word_count.items()]\n",
    "word_list.sort(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 10\n",
    "WINDOW = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选词\n",
    "word2id = {\"UNK\": 0}\n",
    "\n",
    "top_num = len(word_list)*10//100\n",
    "stop_list = word_list[:top_num] + word_list[-top_num:]\n",
    "stop_set = set([k for k, _ in stop_list])\n",
    "word2id =  {k:i+1 for i, (k, _) in  enumerate(word_list[top_num:-top_num])}\n",
    "word2id[\"UNK\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2id)\n",
    "EMBED_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "将文本中的词进行转换，转换成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(data)\n",
    "context_data = []\n",
    "label_data = []\n",
    "temp_windows = [\"P\"*WINDOW]\n",
    "for i, d in enumerate(data):\n",
    "    temp_windows.append(d)\n",
    "    \n",
    "    context = []\n",
    "    if len(temp_windows) == 2*WINDOW+1:\n",
    "        for j, t in enumerate(temp_windows):\n",
    "            if t == \"P\":\n",
    "                continue\n",
    "            if j == WINDOW:\n",
    "                continue\n",
    "            context.append(word2id.get(t, 0))\n",
    "        label.append(word2id.get(temp_windows[WINDOW], 0))\n",
    "        temp_windows.pop(0)\n",
    "    \n",
    "    if context:\n",
    "        context_data.append(context)\n",
    "        label_data.append(label[0])\n",
    "            \n",
    "    if i == data_len-1:\n",
    "        context = []\n",
    "        for j in range(WINDOW):\n",
    "            \n",
    "            \n",
    "            for k, t in enumerate(temp_windows[j:]):\n",
    "                if k == WINDOW:\n",
    "                    continue\n",
    "                context.append(word2id.get(t, 0))\n",
    "            \n",
    "            label_data.append(word2id.get(temp_windows[j+WINDOW], 0))\n",
    "            \n",
    "            context_data.append(context)\n",
    "#             label_data.append(label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = [tf.keras.utils.to_categorical(d).sum(axis=0) for d in context_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = tf.keras.preprocessing.sequence.pad_sequences(context_data, maxlen=VOCAB_SIZE, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = context_data.astype('float64')\n",
    "label_data = np.array(label_data).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(context_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练专用数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((context_data, label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(data_len).batch(100, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBOW(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embed = tf.keras.layers.Dense(embed_size)\n",
    "        self.out = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "        \n",
    "    \n",
    "    def call(self, input_data):\n",
    "        \n",
    "        x = self.embed(input_data)\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(VOCAB_SIZE, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(tf.constant([[0, 1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 321])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(input_x, input_target):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit = model(input_x)\n",
    "        loss = loss_func(input_target, logit)\n",
    "    \n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimiser.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 321])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer cbow_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = model(train_x)\n",
    "# loss = loss_func(input_target, logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 loss value is 5.771358013153076\n",
      "epoch 1 batch 0 loss value is 5.771195888519287\n",
      "epoch 2 batch 0 loss value is 5.770995616912842\n",
      "epoch 3 batch 0 loss value is 5.770693302154541\n",
      "epoch 4 batch 0 loss value is 5.77040958404541\n",
      "epoch 5 batch 0 loss value is 5.7698893547058105\n",
      "epoch 6 batch 0 loss value is 5.769169807434082\n",
      "epoch 7 batch 0 loss value is 5.7680253982543945\n",
      "epoch 8 batch 0 loss value is 5.766364097595215\n",
      "epoch 9 batch 0 loss value is 5.763433456420898\n",
      "epoch 10 batch 0 loss value is 5.758155345916748\n",
      "epoch 11 batch 0 loss value is 5.748624801635742\n",
      "epoch 12 batch 0 loss value is 5.725463390350342\n",
      "epoch 13 batch 0 loss value is 5.681973934173584\n",
      "epoch 14 batch 0 loss value is 5.597226619720459\n",
      "epoch 15 batch 0 loss value is 5.429547309875488\n",
      "epoch 16 batch 0 loss value is 5.226442337036133\n",
      "epoch 17 batch 0 loss value is 5.0365986824035645\n",
      "epoch 18 batch 0 loss value is 4.912534236907959\n",
      "epoch 19 batch 0 loss value is 4.852176666259766\n",
      "epoch 20 batch 0 loss value is 4.824949741363525\n",
      "epoch 21 batch 0 loss value is 4.81359338760376\n",
      "epoch 22 batch 0 loss value is 4.801486492156982\n",
      "epoch 23 batch 0 loss value is 4.801541805267334\n",
      "epoch 24 batch 0 loss value is 4.797153472900391\n",
      "epoch 25 batch 0 loss value is 4.792986869812012\n",
      "epoch 26 batch 0 loss value is 4.790414810180664\n",
      "epoch 27 batch 0 loss value is 4.798145771026611\n",
      "epoch 28 batch 0 loss value is 4.786746978759766\n",
      "epoch 29 batch 0 loss value is 4.786019802093506\n",
      "epoch 30 batch 0 loss value is 4.786840438842773\n",
      "epoch 31 batch 0 loss value is 4.784079551696777\n",
      "epoch 32 batch 0 loss value is 4.7832560539245605\n",
      "epoch 33 batch 0 loss value is 4.783341884613037\n",
      "epoch 34 batch 0 loss value is 4.783751487731934\n",
      "epoch 35 batch 0 loss value is 4.7947211265563965\n",
      "epoch 36 batch 0 loss value is 4.782674312591553\n",
      "epoch 37 batch 0 loss value is 4.782029628753662\n",
      "epoch 38 batch 0 loss value is 4.7822794914245605\n",
      "epoch 39 batch 0 loss value is 4.781320571899414\n",
      "epoch 40 batch 0 loss value is 4.781223297119141\n",
      "epoch 41 batch 0 loss value is 4.780618667602539\n",
      "epoch 42 batch 0 loss value is 4.780919075012207\n",
      "epoch 43 batch 0 loss value is 4.780298233032227\n",
      "epoch 44 batch 0 loss value is 4.780905246734619\n",
      "epoch 45 batch 0 loss value is 4.7799763679504395\n",
      "epoch 46 batch 0 loss value is 4.779839992523193\n",
      "epoch 47 batch 0 loss value is 4.780056953430176\n",
      "epoch 48 batch 0 loss value is 4.779903411865234\n",
      "epoch 49 batch 0 loss value is 4.779496669769287\n",
      "epoch 50 batch 0 loss value is 4.779268264770508\n",
      "epoch 51 batch 0 loss value is 4.77949333190918\n",
      "epoch 52 batch 0 loss value is 4.779731273651123\n",
      "epoch 53 batch 0 loss value is 4.789071083068848\n",
      "epoch 54 batch 0 loss value is 4.779245376586914\n",
      "epoch 55 batch 0 loss value is 4.778963088989258\n",
      "epoch 56 batch 0 loss value is 4.7889180183410645\n",
      "epoch 57 batch 0 loss value is 4.778669834136963\n",
      "epoch 58 batch 0 loss value is 4.779260158538818\n",
      "epoch 59 batch 0 loss value is 4.7794575691223145\n",
      "epoch 60 batch 0 loss value is 4.778321266174316\n",
      "epoch 61 batch 0 loss value is 4.778433322906494\n",
      "epoch 62 batch 0 loss value is 4.778172969818115\n",
      "epoch 63 batch 0 loss value is 4.778716564178467\n",
      "epoch 64 batch 0 loss value is 4.778534412384033\n",
      "epoch 65 batch 0 loss value is 4.788209438323975\n",
      "epoch 66 batch 0 loss value is 4.778443813323975\n",
      "epoch 67 batch 0 loss value is 4.778547286987305\n",
      "epoch 68 batch 0 loss value is 4.778225421905518\n",
      "epoch 69 batch 0 loss value is 4.778083324432373\n",
      "epoch 70 batch 0 loss value is 4.788734436035156\n",
      "epoch 71 batch 0 loss value is 4.778168201446533\n",
      "epoch 72 batch 0 loss value is 4.778156757354736\n",
      "epoch 73 batch 0 loss value is 4.778041839599609\n",
      "epoch 74 batch 0 loss value is 4.777989864349365\n",
      "epoch 75 batch 0 loss value is 4.778329372406006\n",
      "epoch 76 batch 0 loss value is 4.777981758117676\n",
      "epoch 77 batch 0 loss value is 4.777945041656494\n",
      "epoch 78 batch 0 loss value is 4.777994155883789\n",
      "epoch 79 batch 0 loss value is 4.777846336364746\n",
      "epoch 80 batch 0 loss value is 4.777857780456543\n",
      "epoch 81 batch 0 loss value is 4.77797269821167\n",
      "epoch 82 batch 0 loss value is 4.778039932250977\n",
      "epoch 83 batch 0 loss value is 4.777851104736328\n",
      "epoch 84 batch 0 loss value is 4.777829170227051\n",
      "epoch 85 batch 0 loss value is 4.7777605056762695\n",
      "epoch 86 batch 0 loss value is 4.777637958526611\n",
      "epoch 87 batch 0 loss value is 4.777736663818359\n",
      "epoch 88 batch 0 loss value is 4.777669906616211\n",
      "epoch 89 batch 0 loss value is 4.777802467346191\n",
      "epoch 90 batch 0 loss value is 4.787773609161377\n",
      "epoch 91 batch 0 loss value is 4.777668476104736\n",
      "epoch 92 batch 0 loss value is 4.777609825134277\n",
      "epoch 93 batch 0 loss value is 4.777752876281738\n",
      "epoch 94 batch 0 loss value is 4.777646064758301\n",
      "epoch 95 batch 0 loss value is 4.777648448944092\n",
      "epoch 96 batch 0 loss value is 4.777590751647949\n",
      "epoch 97 batch 0 loss value is 4.777551651000977\n",
      "epoch 98 batch 0 loss value is 4.777566432952881\n",
      "epoch 99 batch 0 loss value is 4.777576923370361\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    \n",
    "    for k, (train_x, train_y) in enumerate(dataset):\n",
    "        \n",
    "        loss = train_step(train_x, train_y)\n",
    "        \n",
    "        if k % 100 == 0:\n",
    "            print(\"epoch {0} batch {1} loss value is {2}\".format(i, k, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
