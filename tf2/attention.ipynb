{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention\n",
    "\n",
    "attention 最早是应用于计算机视觉，随后在NLP领域中应用。随着\n",
    "GPT和BERT的大火，attention也随之得到了关注。\n",
    "attention的核心就是从关注全部到关注重点。\n",
    "\n",
    "attention两个优点\n",
    "1 参数少\n",
    "2 速度快\n",
    "\n",
    "attention 可以解决rnn不能并行计算的问题，每一步计算不依赖于上一步的计算结果。\n",
    "\n",
    "\n",
    "\n",
    "早期attention引入nlp中，是和rnn相结合，应用于Machine translation中。对于NMT任务，一般使用seq2seq模型，或者说是encoder-decoder模型。其流程是讲输入语句encoder一个向量，然后通过decoder进行解码，最后输出目标语句。\n",
    "\n",
    "这种方法存在一些问题，RNN中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度增加，这种结构的效果会显著下降。\n",
    "\n",
    "attenion 的结构如下\n",
    "\n",
    "![title](img/attention_v1.png)\n",
    "\n",
    "attention + RNN做NMT的流程如下：\n",
    "1）利用RNN结构得到encoder的hidden state(h1, h2, ...hT)\n",
    "2）假设当前的decoder的hidden state是s_t-1, 我们可以计算每一个输入位置j与当前输出位置的关联性，$e_{ij}=a(s_{t-1}, h_j)$,写成相应的向量形式即为\n",
    "$\\overrightarrow{e_t}=(a(s_{t-1}, h_1),...,a(s_{t-1}, h_T))$, 其$a$是一种相关性的算符，例如常见的有点乘形式$\\overrightarrow{e_t}=\\overrightarrow{s_{t-1}}\\overrightarrow{h}$, 加权点乘$\\overrightarrow{e_t}=\\overrightarrow{s_{t-1}}W\\overrightarrow{h}$, 加和$\\overrightarrow{e_t}=\\overrightarrow{v}tanh(W_1\\overrightarrow{h}+W_2\\overrightarrow{s_{t-1}})$\n",
    "\n",
    "对于$\\overrightarrow{t}$进行softmax操作将其normalize得到attention的分布，$\\overrightarrow{\\alpha_t}=softmax(\\overrightarrow{e_t})$, 展开形式为$\\alpha_{tj}=\\frac{exp(e_{tj})}{\\sum_{k=1}^{T}{exp(e_{tk})}}$\n",
    "\n",
    "利用$\\overrightarrow{\\alpha_t}$我们可以进行加权求和得到相应的context vector $\\overrightarrow{c_t}=\\sum_{j=1}^{T}{\\alpha_{tj}h_j}$\n",
    "\n",
    "由此，我们可以计算decoder的下一个hidden state $s_t=f(s_{t-1}, y_{t-1}, c_t)$以及该位置的输入$p(y_t|y_1,...,y_{t-1},\\overrightarrow{x})=g(y_{i-1}, s_i, c_i)$\n",
    "\n",
    "这里的关键操作是计算encoder 和 decoder state 之间的关联的权重，得到attention 分布，从而对于当前输出位置得到比较重要的输入位置的权重，在预测输出时相应的会占较大的比重。\n",
    "\n",
    "## attention 原理【1】\n",
    "![title](img/attention_v2.png)\n",
    "\n",
    "对于每一个query, 第一步首先计算query和每个一个key的相似度，得到权值，第二步，将权值经过softmax 归一化得到权重。第三步，将权重与对应的value进行加权平均。\n",
    "\n",
    "attention 有很多不同的类型，\n",
    "\n",
    "![title](img/attention_v3.png)\n",
    "\n",
    "## 1 按照计算区域划分，\n",
    "\n",
    "- soft-attention: 对所有可以求相似度权重，每个可以都有一个对应的权重\n",
    "- hard-attention: 这种方式直接精准定位到某个key, 其余key就都不管了，相当于这个key的概率是1， 其余key的概率全部是0， 因此，这种对齐方式要求很高，要求一步到位，而且不可导，一般需要用强化学习的方法进行训练\n",
    "- local-attention: 这种方式其实是soft-attenion和hard-attention的综合，首选使用attention的方式定位到一个小的区域，然后在这个小区域内用soft-attention.\n",
    "\n",
    "按所用信息划分\n",
    "- general-attenion\n",
    "- self-attention\n",
    "\n",
    "按照使用模型划分\n",
    "- CNN+attention\n",
    "- LSTM+Attention\n",
    "\n",
    "按照模型结构划分\n",
    "- 单层attention\n",
    "- 多层attention\n",
    "- 多头attention\n",
    "\n",
    "按照权值计算方式划分\n",
    "- 点乘： 对应元素相乘\n",
    "- 矩阵相乘 $s(q,k)=q{^T}k$\n",
    "- 余弦相似度 $s(q,k)=\\frac{q{^T}k}{||q||.||k||}$\n",
    "- 串联concatenate： $s(q,k)=W[q,k]$\n",
    "- 多层感知机MLP: $s(q,k)=v{^T_\\alpha}tanh(W_q+U_k)$\n",
    "\n",
    "\n",
    "\n",
    "[1] 参考资料\n",
    "（1） Attention专题 https://zhuanlan.zhihu.com/p/104677204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cmn.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "en = []\n",
    "cn = []\n",
    "for x in data.split(\"\\n\"):\n",
    "    if len(x.split(\"\\t\")) < 2:\n",
    "        continue\n",
    "    ei, ci = x.split(\"\\t\")\n",
    "    ei = ei.lower()\n",
    "    ei = re.sub(r\"([.?,])\", r\" \\1\", ei)\n",
    "    \n",
    "    en.append(re.split(r\"\\s\", ei))\n",
    "    cn.append([c for c in ci])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2id = {\"<start>\": 1, \"<pad>\": 0}\n",
    "cn_word2id = {\"<end>\": 1, \"<start>\": 2, \"<pad>\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en = []\n",
    "for ei in en:\n",
    "    input_en.append(ei)\n",
    "    \n",
    "    for e in ei:\n",
    "        if e not in en_word2id:\n",
    "            en_word2id[e] = len(en_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cn = []\n",
    "target_cn = []\n",
    "for ci in cn:\n",
    "    input_cn.append([\"<start>\"]+ci)\n",
    "    target_cn.append(ci+[\"<end>\"])\n",
    "    \n",
    "    for c in ci:\n",
    "        if c not in cn_word2id:\n",
    "            cn_word2id[c] = len(cn_word2id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en_id = [[en_word2id[e] for e in ei] for ei in input_en]\n",
    "input_cn_id = [[cn_word2id[c] for c in ci] for ci in input_cn]\n",
    "target_cn_id = [[cn_word2id[c] for c in ci] for ci in target_cn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_en_id_pad = tf.keras.preprocessing.sequence.pad_sequences(input_en_id, maxlen=64, padding=\"post\")\n",
    "input_cn_id_pad = tf.keras.preprocessing.sequence.pad_sequences(input_cn_id, maxlen=64, padding=\"post\")\n",
    "target_cn_id_pad = tf.keras.preprocessing.sequence.pad_sequences(target_cn_id, maxlen=64, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_en_id_pad, input_cn_id_pad, target_cn_id_pad))\n",
    "dataset = dataset.shuffle(100).batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDING_SIZE = 10\n",
    "EN_VOCAB_SIZE = len(en_word2id)\n",
    "CN_VOCAB_SIZE = len(cn_word2id)\n",
    "\n",
    "LSTM_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embed = tf.keras.layers.Embedding(EN_VOCAB_SIZE, EMBEDING_SIZE)\n",
    "        self.lstm = tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=True, return_state=True)\n",
    "        \n",
    "    def call(self, input_x):\n",
    "        x = self.word_embed(input_x)\n",
    "        \n",
    "        x, h_state, c_state = self.lstm(x)\n",
    "        \n",
    "        return x, h_state, c_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embed = tf.keras.layers.Embedding(CN_VOCAB_SIZE, EMBEDING_SIZE)\n",
    "        self.lstm = tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=True, return_state=True)\n",
    "        self.out = tf.keras.layers.Dense(CN_VOCAB_SIZE, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, input_x, input_state):\n",
    "        x = self.word_embed(input_x)\n",
    "        \n",
    "        \n",
    "        output, h_state, c_state = self.lstm(x, initial_state=input_state)\n",
    "        \n",
    "        logits = self.out(output)\n",
    "        \n",
    "        return logits, h_state, c_state, x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h_state, c_state = encoder(tf.constant([[1, 2],[2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()\n",
    "dout, dh_state, dc_state, dx = decoder(tf.constant([[1, 2],[2, 3]]), [h_state, c_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dx[:,0:1,:]\n",
    "b = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1355, shape=(1, 1), dtype=int32, numpy=array([[8]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1, 2]])\n",
    "b = tf.constant([[2, 3]])\n",
    "b= tf.transpose(b)\n",
    "tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.matmul(a, tf.transpose(b, perm=[0, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2132, shape=(2, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.49975556, 0.50024444]],\n",
       "\n",
       "       [[0.49998578, 0.50001425]]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.softmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
