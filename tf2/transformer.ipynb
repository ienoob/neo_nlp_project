{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer\n",
    "attention 由由bengio团队于2014年提出， 并在近年来得到广泛的应用。\n",
    "transformer中抛弃了传统CNN和RNN, 整个网络结构完全式由attention机制组成。准确来讲，transfoerm 由且仅由self-attetion 和feed forward neural network 组成。一个基于transformer的形式进行搭建。\n",
    "\n",
    "对于RNN等类型的网络，其计算是顺序的，这样就存在两个问题\n",
    "1）时间片t的计算依赖t-1时刻的计算结果，泽洋限制了模型的并行能力\n",
    "2）顺序计算的过程中，信息会丢失，尽管有了LSTM的门机制来缓解长期依赖的问题，但是对于特别长期的信息就无能为力了。\n",
    "\n",
    "transfomer针对这两个问题，采取了相应的措施。首先使用attenion机制，将序列中的任意两个位置之间的距离缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性。\n",
    "\n",
    "transformer 本质上是一个encoder decoder 的结构，那么\n",
    "![title](img/transformer_v1.png)\n",
    "\n",
    "如论文中所设置的，编码器由六个编码block组成，解码器器同样由6个解码器block组成，与所有的生成模型相同的是，编码器的输出会作为解码器的输入。如下图所示：\n",
    "![title](img/transformer_v2.png)\n",
    "\n",
    "对于encoder 模型， 数据首先会经过一个叫做“self-attention”的模块得到一个加权之后的特征向量Z, \n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "得到Z之后，它会被送到encoder的下一个模块， 即feed forward neural network, 也就是全连接层。该网络一共两层，第一层激活函数是Relu, 第二层是线性激活函数，可以表示为：\n",
    "\n",
    "$$FFN(Z)=max(0, ZW_1+b_1)W_2+b_2$$\n",
    "\n",
    "decoder 结构如下图所示，他和encoder的不同之处在于decoder多了一个encoder-decoder attention , 两个attention分别用于计算输入和输出的权值。\n",
    "\n",
    "1 self-attention: 当前翻译和已经翻译的前文之间的关系\n",
    "2 encoder-decoder attention: 当前翻译和编码的特征向量之间的关系。\n",
    "\n",
    "![title](img/transformer_v3.png)\n",
    "\n",
    "输入编码\n",
    "\n",
    "首先使用word2vec 将词转化为向量，在最底层的block中，$x$将直接作为transformer的输入，而在其他层中，输入则是上一个block的输出。\n",
    "\n",
    "![title](img/transformer_v4.png)\n",
    "\n",
    "self-attention\n",
    "\n",
    "self-attention 是 transformer最核心的内容。在self-attention中，每个单词有3个不同的向量，它们分别是Query 向量（Q）, key向量（K）和value 向量（V）, 长度均是64. 它们是通过3个不同的权值矩阵由嵌入向量乘以三个不同的权值矩阵$W^Q$, $W^K$, $W^V$得到， 其中三个矩阵的尺寸也是相同的。\n",
    "\n",
    "那么Query\\Key\\value的意义是啥？\n",
    "\n",
    "- 将输入单词转化为嵌入向量\n",
    "- 根据嵌入向量得到q, k, v三个向量\n",
    "- 为每个向量计算一个score $score=q.k$\n",
    "- 为了梯度的稳定，transformer 使用了score归一化， 即除以$\\sqrt{d_k}$\n",
    "- 对score施以softmax 激活函数\n",
    "- softmax点乘value值v, 得到加权的每个输入向量的评分v\n",
    "- 相加之后得到最终的输出结果z: $z=\\sum{v}$\n",
    "\n",
    "self-attention 最后一点采用了残差网络的short-cut结构，解决网络退化问题。\n",
    "\n",
    "![title](img/transformer_v5.png)\n",
    "\n",
    "## Multi-Head attention\n",
    "multi-head attention 相当于h个不同的self-attention的集成， 在这里我们以h=8举例说明。\n",
    "\n",
    "- 根据X分别输入8个self-attention中，得到8个加权后的特征矩阵， $Z_i,i\\in{i,2,...,8}$\n",
    "- 将8个$Z_i按列拼成一个大的特征矩阵$\n",
    "- 特征矩阵经过一层全连接后得到输出Z\n",
    "\n",
    "过程如下图所示\n",
    "![titile](img/transformer_v6.png)\n",
    "\n",
    "同时这里也加入了short-cut机制\n",
    "\n",
    "## Encoder-Decoder Attention\n",
    "\n",
    "\n",
    "在解码器中，Transformer block 比编码器中多了个encoder-decoder attention. 在encoder decoder attention 中， Q来自于解码器的上一个输出，K和V则来自于编码器的输出，其计算方式完全和上面的计算方式相同。\n",
    "\n",
    "## 损失层\n",
    "\n",
    "解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的概率输出向量。我们可以通过CTC等损失函数训练模型。\n",
    "\n",
    "一个完整可训练的网络结构便是encoder和decoder的堆叠（各N个）。我们可以得到完整的transformer的结构。\n",
    "\n",
    "![title](img/transformer_v7.png)\n",
    "\n",
    "## 位置编码\n",
    "\n",
    "transformer模型对顺序序列的处理能力，也就是说无论句子的结构怎么达伦，transformer都会得到类似的结果。\n",
    "\n",
    "怎么解决位置信息？\n",
    "a 根据数据学习\n",
    "b 自己设计编码规则。\n",
    "\n",
    "原论文给出公式\n",
    "\n",
    "$$PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE(pos,2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "在上面的式子中，pos表示单词的位置，i表示单词的维度，\n",
    "\n",
    "根据公式$sin(\\alpha+\\beta)=sin{\\alpha}cos{\\beta}+cos{\\alpha}sin{\\beta}$以及$cos(\\alpha+\\beta)=cos{\\alpha}cos{\\beta}-sin{\\alpha}sin{\\beta}$， 这表明位置k+p的位置向量可以表示为位置k的特征向量的线性变化。\n",
    "\n",
    "\n",
    "## transformer的优缺点\n",
    "优点：\n",
    "(1) 算法设计足够创新, 抛弃了RNN或CNN来做NLP\n",
    "(2)将任意两个单词的距离设为1，有效解决了长期依赖问题\n",
    "(3)其应用范围不限于机器翻译\n",
    "(4)并行性能好\n",
    "\n",
    "缺点：\n",
    "（1）虽然不使用RNN和CNN，但是丧失了捕捉局部特征的能力\n",
    "（2）transformer对于位置信息的处理不够完善\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t 是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    # 这里的i等价与上面公式中的2i和2i+1\n",
    "    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    # 第2i项使用sin\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    # 第2i+1项使用cos\n",
    "    cones = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7, shape=(2, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mark(seq):\n",
    "    # 获取为0的padding项\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # 扩充维度以便用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)\n",
    "\n",
    "# mark 测试\n",
    "create_padding_mark([[1,2,0,0,3],[3,4,5,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mark(size):\n",
    "    # 1 - 对角线和取下三角的全部对角线（-1->全部）\n",
    "    # 这样就可以构造出每个时刻未预测token的掩码\n",
    "    mark = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mark  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                              as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "(pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=8087>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=40\n",
    "def filter_long_sent(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .', shape=(), dtype=string)\n",
      "tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n",
      "[8214, 6, 40, 4092, 57, 3, 1687, 1, 6155, 12, 3, 461, 6770, 19, 5227, 1088, 97, 1, 5, 8, 3, 4213, 3408, 7256, 1670, 2, 8215]\n",
      "[8087, 4, 59, 15, 1792, 6561, 3060, 7952, 1, 15, 103, 134, 378, 3, 47, 6122, 6, 5311, 1, 91, 13, 1849, 559, 1609, 894, 2, 8088]\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_examples:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    x, y = encode(x, y)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "        lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "        lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 使用.map()运行相关图操作\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "# 过滤过长的数据\n",
    "train_dataset = train_dataset.filter(filter_long_sent)\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 验证集数据\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_long_sent).padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None, 40), (None, 40)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # query key 相乘获取匹配关系\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # 掩码\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    # 通过softmax获取attention权重\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    # attention 乘上value\n",
    "    output = tf.matmul(attention_weights, v) # （.., seq_len_v, depth）\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_att = scaled_dot_product_attention(\n",
    "    q, k, v, None)\n",
    "    print('attention weight:')\n",
    "    print(temp_att)\n",
    "    print('output:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 显示为numpy类型\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 3)\n",
    "# 关注第2个key, 返回对应的value\n",
    "temp_q = tf.constant([[0,10,0]], dtype=tf.float32)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 依次放入每个query\n",
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutilHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MutilHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # d_model 必须可以正确分为各个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头后的维度\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头, 将头个数的维度 放到 seq_len 前面\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 分头前的前向网络，获取q、k、v语义\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 通过缩放点积注意力层\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "        # 合并多头\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512) (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MutilHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))\n",
    "output, att = temp_mha(y, k=y, q=y, mask=None)\n",
    "print(output.shape, att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, diff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(diff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_fnn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_fnn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        self.eps = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ddf, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MutilHeadAttention(d_model, n_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ddf)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        # 多头注意力网络\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # 前向网络\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, drop_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MutilHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MutilHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(drop_rate)\n",
    "        self.dropout2 = layers.Dropout(drop_rate)\n",
    "        self.dropout3 = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self,inputs, encode_out, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "        # masked muti-head attention\n",
    "        att1, att_weight1 = self.mha1(inputs, inputs, inputs,look_ahead_mask)\n",
    "        att1 = self.dropout1(att1, training=training)\n",
    "        out1 = self.layernorm1(inputs + att1)\n",
    "        # muti-head attention\n",
    "        att2, att_weight2 = self.mha2(encode_out, encode_out, inputs, padding_mask)\n",
    "        att2 = self.dropout2(att2, training=training)\n",
    "        out2 = self.layernorm2(out1 + att2)\n",
    "        \n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_out)\n",
    "        \n",
    "        return out3, att_weight1, att_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "sample_decoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                input_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.encode_layer = [EncoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                            for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "    def call(self, inputs, training, mark):\n",
    "        \n",
    "        seq_len = inputs.shape[1]\n",
    "        word_emb = self.embedding(inputs)\n",
    "        word_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_emb + self.pos_embedding[:,:seq_len,:]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encode_layer[i](x, training, mark)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 120, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder = Encoder(2, 512, 8, 1024, 5000, 200)\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 120)),\n",
    "                                      False, None)\n",
    "sample_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                target_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.decoder_layers= [DecoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                             for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self, inputs, encoder_out,training,\n",
    "             look_ahead_mark, padding_mark):\n",
    "    \n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "        h = self.embedding(inputs)\n",
    "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        h += self.pos_embedding[:,:seq_len,:]\n",
    "        \n",
    "        h = self.dropout(h, training=training)\n",
    "#         print('--------------------\\n',h, h.shape)\n",
    "        # 叠加解码层\n",
    "        for i in range(self.n_layers):\n",
    "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
    "                                                   training, look_ahead_mark,\n",
    "                                                   padding_mark)\n",
    "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
    "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
    "        \n",
    "        return h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 100, 512]), TensorShape([64, 8, 100, 100]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(2, 512,8,1024,5000, 200)\n",
    "sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),\n",
    "                                            sample_encoder_output, False,\n",
    "                                            None, None)\n",
    "sample_decoder_output.shape, attn['decoder_layer1_att_w1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, diff,\n",
    "                input_vocab_size, target_vocab_size,\n",
    "                max_seq_len, drop_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads,diff,\n",
    "                              input_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, diff,\n",
    "                              target_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs, targets, training, encode_padding_mask, \n",
    "            look_ahead_mask, decode_padding_mask):\n",
    "        \n",
    "        encode_out = self.encoder(inputs, training, encode_padding_mask)\n",
    "        print(encode_out.shape)\n",
    "        decode_out, att_weights = self.decoder(targets, encode_out, training, \n",
    "                                               look_ahead_mask, decode_padding_mask)\n",
    "        print(decode_out.shape)\n",
    "        final_out = self.final_layer(decode_out)\n",
    "        \n",
    "        return final_out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n",
      "(64, 26, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "n_layers=2, d_model=512, n_heads=8, diff=1024,\n",
    "input_vocab_size=8500, target_vocab_size=8000, max_seq_len=120\n",
    ")\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                              encode_padding_mask=None,\n",
    "                               look_ahead_mask=None,\n",
    "                               decode_padding_mask=None,\n",
    "                              )\n",
    "fn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "max_seq_len = 40\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, \n",
    "                                    beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'train step')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试\n",
    "temp_learing_rate = CustomSchedule(d_model)\n",
    "plt.plot(temp_learing_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('train step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size,\n",
    "                          max_seq_len, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码\n",
    "def create_mask(inputs,targets):\n",
    "    encode_padding_mask = create_padding_mark(inputs)\n",
    "    # 这个掩码用于掩输入解码层第二层的编码层输出\n",
    "    decode_padding_mask = create_padding_mark(inputs)\n",
    "    \n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "    \n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return encode_padding_mask, combine_mask, decode_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = './checkpoint/train'\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    tar_inp = targets[:,:-1]\n",
    "    tar_real = targets[:,1:]\n",
    "    # 构造掩码\n",
    "    encode_padding_mask, combined_mask, decode_padding_mask = create_mask(inputs, tar_inp)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inputs, tar_inp,\n",
    "                                    True,\n",
    "                                    encode_padding_mask,\n",
    "                                    combined_mask,\n",
    "                                    decode_padding_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40, 128)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "(64, 39, 128)\n",
      "epoch 1, batch 0, loss:3.6524, acc:0.0000\n",
      "epoch 1, batch 500, loss:3.4110, acc:0.0359\n",
      "(31, 40, 128)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x0000028939C45550>>, which Python reported as:\n",
      "    def call(self, inputs, encoder_out,training,\n",
      "             look_ahead_mark, padding_mark):\n",
      "    \n",
      "        seq_len = tf.shape(inputs)[1]\n",
      "        attention_weights = {}\n",
      "        h = self.embedding(inputs)\n",
      "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
      "        h += self.pos_embedding[:,:seq_len,:]\n",
      "        \n",
      "        h = self.dropout(h, training=training)\n",
      "#         print('--------------------\\n',h, h.shape)\n",
      "        # 叠加解码层\n",
      "        for i in range(self.n_layers):\n",
      "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
      "                                                   training, look_ahead_mark,\n",
      "                                                   padding_mark)\n",
      "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
      "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
      "        \n",
      "        return h, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "(31, 39, 128)\n",
      "epoch 1, loss:3.2020, acc:0.0476\n",
      "time in 1 epoch:872.7552173137665 secs\n",
      "\n",
      "epoch 2, batch 0, loss:2.2260, acc:0.0869\n",
      "epoch 2, batch 500, loss:2.3102, acc:0.1124\n",
      "epoch 2, save model at ./checkpoint/train\\ckpt-1\n",
      "epoch 2, loss:2.2618, acc:0.1177\n",
      "time in 1 epoch:6128.533282756805 secs\n",
      "\n",
      "epoch 3, batch 0, loss:1.8208, acc:0.1306\n",
      "epoch 3, batch 500, loss:2.0230, acc:0.1404\n",
      "epoch 3, loss:1.9956, acc:0.1440\n",
      "time in 1 epoch:880.405948638916 secs\n",
      "\n",
      "epoch 4, batch 0, loss:1.6332, acc:0.1522\n",
      "epoch 4, batch 500, loss:1.7974, acc:0.1668\n",
      "epoch 4, save model at ./checkpoint/train\\ckpt-2\n",
      "epoch 4, loss:1.7667, acc:0.1713\n",
      "time in 1 epoch:836.4725286960602 secs\n",
      "\n",
      "epoch 5, batch 0, loss:1.4163, acc:0.1795\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # inputs 葡萄牙语， targets英语\n",
    "    \n",
    "    for batch, (inputs, targets) in enumerate(train_dataset):\n",
    "        # 训练\n",
    "        train_step(inputs, targets)\n",
    "        \n",
    "        if batch % 500 == 0:\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "            epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
    "            ))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch {}, save model at {}'.format(\n",
    "        epoch+1, ckpt_save_path\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "    epoch+1, train_loss.result(), train_accuracy.result()\n",
    "    ))\n",
    "    \n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1, 2, 3],[4, 5, 6]]\n",
    "list(zip(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"M\"],\n",
    " [\"E\",\"E\",\"M\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"M\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\",\"E\"],\n",
    " [\"E\",\"E\",\"M\",\"M\",\"E\",\"E\",\"E\",\"E\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
