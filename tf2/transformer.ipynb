{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer\n",
    "attention 由由bengio团队于2014年提出， 并在近年来得到广泛的应用。\n",
    "transformer中抛弃了传统CNN和RNN, 整个网络结构完全式由attention机制组成。准确来讲，transfoerm 由且仅由self-attetion 和feed forward neural network 组成。一个基于transformer的形式进行搭建。\n",
    "\n",
    "对于RNN等类型的网络，其计算是顺序的，这样就存在两个问题\n",
    "1）时间片t的计算依赖t-1时刻的计算结果，泽洋限制了模型的并行能力\n",
    "2）顺序计算的过程中，信息会丢失，尽管有了LSTM的门机制来缓解长期依赖的问题，但是对于特别长期的信息就无能为力了。\n",
    "\n",
    "transfomer针对这两个问题，采取了相应的措施。首先使用attenion机制，将序列中的任意两个位置之间的距离缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性。\n",
    "\n",
    "transformer 本质上是一个encoder decoder 的结构，那么\n",
    "![title](img/transformer_v1.png)\n",
    "\n",
    "如论文中所设置的，编码器由六个编码block组成，解码器器同样由6个解码器block组成，与所有的生成模型相同的是，编码器的输出会作为解码器的输入。如下图所示：\n",
    "![title](img/transformer_v2.png)\n",
    "\n",
    "对于encoder 模型， 数据首先会经过一个叫做“self-attention”的模块得到一个加权之后的特征向量Z, \n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "得到Z之后，它会被送到encoder的下一个模块， 即feed forward neural network, 也就是全连接层。该网络一共两层，第一层激活啊哈桑农户是Relu, 第二层是象形激活函数，可以表示为：\n",
    "\n",
    "$$FFN(Z)=max(0, ZW_1+b_1)W_2+b_2$$\n",
    "\n",
    "decoder 结构如下图所示，他和encoder的不同之处在于decoder多了一个encoder-decoder attention , 两个attention分别用于计算输入和输出的权值。\n",
    "\n",
    "1 self-attention: 当前翻译和已经翻译的前文之间的关系\n",
    "2 encoder-decoder attention: 当前翻译和编码的特征向量之间的关系。\n",
    "\n",
    "![title](img/transformer_v3.png)\n",
    "\n",
    "输入编码\n",
    "\n",
    "首先使用word2vec 将词转化为向量，在最底层的block中，$x$将直接作为transformer的输入，而在其他层中，输入则是上一个block的输出。\n",
    "\n",
    "![title](img/transformer_v4.png)\n",
    "\n",
    "self-attention\n",
    "\n",
    "self-attention 是 transformer最核心的内容。在self-attention中，每个单词有3个不同的向量，它们分别是Query 向量（Q）, key向量（K）和value 向量（V）, 长度均是64. 它们是通过3个不同的权值矩阵由嵌入向量乘以三个不同的权值矩阵$W^Q$, $W^K$, $W^V$得到， 其中三个矩阵的尺寸也是相同的。\n",
    "\n",
    "那么Query\\Key\\value的意义是啥？\n",
    "\n",
    "- 将输入单词转化为嵌入向量\n",
    "- 根据嵌入向量得到q, k, v三个向量\n",
    "- 为每个向量计算一个score $score=q.k$\n",
    "- 为了梯度的稳定，transformer 使用了score归一化， 即除以$\\sqrt{d_k}$\n",
    "- 对score施以softmax 激活函数\n",
    "- softmax点乘value值v, 得到加权的每个输入向量的评分v\n",
    "- 相加之后得到最终的输出结果z: $z=\\sum{v}$\n",
    "\n",
    "self-attention 最后一点采用了残差网络的short-cut结构，解决网络退化问题。\n",
    "\n",
    "![title](img/transformer_v5.png)\n",
    "\n",
    "## Multi-Head attention\n",
    "multi-head attention 相当于h个不同的self-attention的集成， 在这里我们以h=8举例说明。\n",
    "\n",
    "- 根据X分别输入8个self-attention中，得到8个加权后的特征矩阵， $Z_i,i\\in{i,2,...,8}$\n",
    "- 将8个$Z_i按列拼成一个大的特征矩阵$\n",
    "- 特征矩阵经过一层全连接后得到输出Z\n",
    "\n",
    "过程如下图所示\n",
    "![titile](img/transformer_v6.png)\n",
    "\n",
    "同时这里也加入了short-cut机制\n",
    "\n",
    "## Encoder-Decoder Attention\n",
    "\n",
    "\n",
    "在解码器中，Transformer block 比编码器中多了个encoder-decoder attention. 在encoder decoder attention 中， Q来自于解码器的上一个输出，K和V则来自于编码器的输出，其计算方式完全和上面的计算方式相同。\n",
    "\n",
    "## 损失层\n",
    "\n",
    "解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的概率输出向量。我们可以通过CTC等损失函数训练模型。\n",
    "\n",
    "一个完整可训练的网络结构便是encoder和decoder的堆叠（各N个）。我们可以得到完整的transformer的结构。\n",
    "\n",
    "![title](img/transformer_v7.png)\n",
    "\n",
    "## 位置编码\n",
    "\n",
    "transformer模型对顺序序列的处理能力，也就是说无论句子的结构怎么达伦，transformer都会得到类似的结果。\n",
    "\n",
    "怎么解决位置信息？\n",
    "a 根据数据学习\n",
    "b 自己设计编码规则。\n",
    "\n",
    "原论文给出公式\n",
    "\n",
    "$$PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_model}}})$$\n",
    "$$PE(pos,2i+1)=cos(\\frac{pos}{10000^{2i}{d_model}})$$\n",
    "\n",
    "在上面的式子中，pos表示单词的位置，i表示单词的维度，\n",
    "\n",
    "根据公式$sin(\\alpha+\\beta)=sin{\\alpha}cos{\\beta}+cos{\\alpha}sin{\\beta}$以及$cos(\\alpha+\\beta)=cos{\\alpha}cos{\\beta}-sin{\\alpha}sin{\\beta}$， 这表明位置k+p的位置向量可以表示为位置k的特征向量的线性变化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t 是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    # 这里的i等价与上面公式中的2i和2i+1\n",
    "    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    # 第2i项使用sin\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    # 第2i+1项使用cos\n",
    "    cones = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 20)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(10, 20)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3, shape=(1, 10, 20), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
       "        [ 8.4147096e-01,  3.8767424e-01,  1.5782665e-01,  6.3053876e-02,\n",
       "          2.5116222e-02,  9.9998331e-03,  3.9810613e-03,  1.5848925e-03,\n",
       "          6.3095731e-04,  2.5118864e-04,  5.4030228e-01,  9.2179644e-01,\n",
       "          9.8746681e-01,  9.9801010e-01,  9.9968451e-01,  9.9994999e-01,\n",
       "          9.9999207e-01,  9.9999875e-01,  9.9999982e-01,  9.9999994e-01],\n",
       "        [ 9.0929741e-01,  7.1471345e-01,  3.1169716e-01,  1.2585682e-01,\n",
       "          5.0216600e-02,  1.9998666e-02,  7.9620592e-03,  3.1697811e-03,\n",
       "          1.2619144e-03,  5.0237728e-04, -4.1614684e-01,  6.9941735e-01,\n",
       "          9.5018148e-01,  9.9204844e-01,  9.9873835e-01,  9.9980003e-01,\n",
       "          9.9996829e-01,  9.9999499e-01,  9.9999923e-01,  9.9999988e-01],\n",
       "        [ 1.4112000e-01,  9.2996645e-01,  4.5775455e-01,  1.8815888e-01,\n",
       "          7.5285293e-02,  2.9995501e-02,  1.1942931e-02,  4.7546616e-03,\n",
       "          1.8928709e-03,  7.5356587e-04, -9.8999250e-01,  3.6764446e-01,\n",
       "          8.8907862e-01,  9.8213857e-01,  9.9716204e-01,  9.9955004e-01,\n",
       "          9.9992865e-01,  9.9998868e-01,  9.9999821e-01,  9.9999970e-01],\n",
       "        [-7.5680250e-01,  9.9976605e-01,  5.9233773e-01,  2.4971211e-01,\n",
       "          1.0030649e-01,  3.9989334e-02,  1.5923614e-02,  6.3395305e-03,\n",
       "          2.5238267e-03,  1.0047544e-03, -6.5364361e-01, -2.1630669e-02,\n",
       "          8.0568975e-01,  9.6832013e-01,  9.9495661e-01,  9.9920011e-01,\n",
       "          9.9987322e-01,  9.9997991e-01,  9.9999684e-01,  9.9999952e-01],\n",
       "        [-9.5892429e-01,  9.1319513e-01,  7.1207315e-01,  3.1027156e-01,\n",
       "          1.2526439e-01,  4.9979169e-02,  1.9904044e-02,  7.9243826e-03,\n",
       "          3.1547814e-03,  1.2559429e-03,  2.8366220e-01, -4.0752259e-01,\n",
       "          7.0210528e-01,  9.5064795e-01,  9.9212337e-01,  9.9875027e-01,\n",
       "          9.9980187e-01,  9.9996859e-01,  9.9999505e-01,  9.9999923e-01],\n",
       "        [-2.7941549e-01,  6.8379402e-01,  8.1395954e-01,  3.6959618e-01,\n",
       "          1.5014327e-01,  5.9964005e-02,  2.3884159e-02,  9.5092161e-03,\n",
       "          3.7857350e-03,  1.5071313e-03,  9.6017027e-01, -7.2967511e-01,\n",
       "          5.8092153e-01,  9.2919248e-01,  9.8866427e-01,  9.9820054e-01,\n",
       "          9.9971473e-01,  9.9995476e-01,  9.9999285e-01,  9.9999887e-01],\n",
       "        [ 6.5698659e-01,  3.4744266e-01,  8.9544296e-01,  4.2744994e-01,\n",
       "          1.7492741e-01,  6.9942847e-02,  2.7863896e-02,  1.1094024e-02,\n",
       "          4.4166869e-03,  1.7583196e-03,  7.5390226e-01, -9.3770123e-01,\n",
       "          4.4517627e-01,  9.0403903e-01,  9.8458135e-01,  9.9755102e-01,\n",
       "          9.9961174e-01,  9.9993849e-01,  9.9999022e-01,  9.9999845e-01],\n",
       "        [ 9.8935825e-01, -4.3251216e-02,  9.5448089e-01,  4.8360252e-01,\n",
       "          1.9960120e-01,  7.9914697e-02,  3.1843189e-02,  1.2678806e-02,\n",
       "          5.0476375e-03,  2.0095077e-03, -1.4550003e-01, -9.9906421e-01,\n",
       "          2.9827204e-01,  8.7528771e-01,  9.7987723e-01,  9.9680173e-01,\n",
       "          9.9949288e-01,  9.9991959e-01,  9.9998724e-01,  9.9999797e-01],\n",
       "        [ 4.1211849e-01, -4.2718029e-01,  9.8959351e-01,  5.3783053e-01,\n",
       "          2.2414905e-01,  8.9878552e-02,  3.5821978e-02,  1.4263555e-02,\n",
       "          5.6785857e-03,  2.2606959e-03, -9.1113025e-01, -9.0416646e-01,\n",
       "          1.4389123e-01,  8.4305298e-01,  9.7455490e-01,  9.9595273e-01,\n",
       "          9.9935818e-01,  9.9989825e-01,  9.9998385e-01,  9.9999744e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
